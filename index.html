<!-- saved from url=(0031)https://csgaobb.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
<title>Bin-Bin Gao's homepage</title>
<link rel="shortcut icon" href="./Imgs_files/binbin_favicon.ico">
<meta content="Bin-Bin Gao, 高斌斌，csgaobb.github.io" name="keywords">
<style media="screen" type="text/css"></style>

<link href="./main_files/homepage.css" rel="stylesheet" type="text/css">
<!--<link rel="stylesheet" href="./main_files/owl.carousel.css">
<link rel="stylesheet" href="./main_files/owl.theme.css">-->

<!-- Google Analytics -->
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-69879514-1']);
    _gaq.push(['_trackPageview']);

    (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>

</script>
<script src='./main_files/hidebib.js' type="text/javascript"></script>
</head>
<!-- Google custom search -->
  <script type="text/javascript">
    (function() {
    var cx = '015564147751910566765:wsger_p3dsg';
    var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
    '//www.google.com/cse/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
    })();
  </script>


<body>
<div class="searchbox">
     <gcse:searchbox-only resultsUrl="./search.html"
           autoCompleteMaxCompletions="5"
           autoCompleteMatchType="any"/>
     </gcse:searchbox-only></br>
    <a href="http://www.nju.edu.cn" target="_blank">
        <img title="Nanjing University"  class="nju" src="./Imgs_files/nju.jpg">
     </a></br>
     <a href="http://lamda.nju.edu.cn/" target="_blank">
     <img title="LAMDA"  class="lamda" src="./Imgs_files/lamda.png">
     </a>
</div>
<div style="margin-bottom: 0em; border: 0px solid #ddd; background-color: #fff; padding: 1em; height: 185px;">
<div style="margin: 0px auto; width: 100%;">
<a href="http://lamda.nju.edu.cn/gaobb" target="_blank">
<!--<img title="Bin-Bin Gao" class="myphoto1" src="./Imgs_files/IMG_5241.JPG">-->
<img title="Bin-Bin Gao" class="myphoto1"  src="./Imgs_files/gaobb.JPG">
</a>
<div style="padding-left: 15em; padding-right: 2em;vertical-align: top; height: 180px;">
<span style="line-height: 100%; font-size: 20pt;padding-top: 5em">Bin-Bin Gao（高斌斌）</span><br>
<span>Ph.D. candidate</span> <br>
<span><a href="http://lamda.nju.edu.cn/">LAMDA Group</a></span> <br>
<span><a href="https://keysoftlab.nju.edu.cn/">National Key Laboratory for Novel Software Technology</a></span> <br>
<span><a href="http://cs.nju.edu.cn/">Department of Computer Science &amp; Technology</a></span> <br>
<span><a href="http://nju.edu.cn/">Nanjing University</a></span> <br>
<span>Supervisor: <a href="http://cs.nju.edu.cn/wujx"> Prof. Jianxin Wu</a></span> <br><br>

<span>Nanjing 210023, China</span><br>
<span><strong>Email: <a href="mailto:csgaobb@gmail.com"></strong>csgaobb@gmail.com</a>  or <a href="mailto:gaob@lamda.nju.edu.cn"></strong>gaobb@lamda.nju.edu.cn</a></span> <br><br>


</div>
</div>
</div>
<hr>


<div style="clear: both;">
<div class="section">
<h2>Biography
<!--[<a href="./main_files/GAOBB_CV.pdf">CV</a>]--></h2>

<div style="text-align:justify;">
<div class="paper" >
I will join <a href="https://bestimage.qq.com/"> Tecent YouTu Lab</a> in July. I have got my Ph.D. in <a href="http://www.nju.edu.cn/">Nanjing University</a> at <a href="http://lamda.nju.edu.cn/">LAMDA Group</a>, led by Prof. <a href="http://lamda.nju.edu.cn/zhouzh/" target="_blank"> Zhi-Hua Zhou</a>, advised by Prof. <a href="http://cs.nju.edu.cn/wujx">Jianxin Wu</a>. Beore that, I got my B.S. degreee in applied mathematics in 2010, and received my M.S. degree in 2013 from <a href="http://swu.edu.cn/">Southwest University</a>, Chongqing China. My research interests include computer vision and machine learning,
especially visual recognition and deep learning. I sever as the reviewer (or PC member) for CVPR, ICCV, AAAI, ECCV, ACCV, NN, TNNLS etc.
</div>
</div>
</div>
</div>


<div style="clear: both;">
<div class="section">
  <h2>News</h2>
  <div class="paper">
    <ul>
    <li> Jun. 21, 2018, I got my Ph.D. in <a href="http://www.nju.edu.cn/">Nanjing University</a> at <a href="http://lamda.nju.edu.cn/">LAMDA Group</a>, advised by Prof. <a href="http://cs.nju.edu.cn/wujx">Jianxin Wu</a>. 
    <li> May 21, 2018, I defended my Ph.D. thesis.
    <li> Apr. 17, 2018: A paper is accepted by <a href="https://www.ijcai-18.org/">IJCAI 2018</a> (Acceptance Rates: 710/3470=20.46%).
    <li> Mar. 28, 2018: A paper (in Chinese) is accepted by <a href="http://engine.scichina.com/publisher/scp/journal/Sci%20Sin%20Info%20F?slug=Overview">SCIENTIA SINICA Informatics</a>. 
    <li> Mar. 26, 2017: A journal paper on deep label distribution learning is accepted by <a href="http://signalprocessingsociety.org/publications-resources/ieee-transactions-image-processing"> IEEE Trans. Image Processing </a>.</li>
    <li> Dec. 11-17, 2015: Attended <a href="http://pamitc.org/iccv15/"> ICCV 2015</a>.</li>
    <li> Nov. 13, 2015: A paper accepted for <a href="http://aaai.org/"> AAAI 2016</a>. </li>
    <li> Oct. 10, 2015: Two papers accepted for ICCV 2015 Workshop. </li>
    <li> Sep. 20, 2015: First runner-up in <a href="http://gesture.chalearn.org/"> Cultural Event Recognition </a> at ICCV 2015. (with X.-S Wei and J. Wu)</li>
    <!--<li> Sep. 20, 2015: The fourth place in <a href="http://gesture.chalearn.org/"> Apprament Age Estimation </a> at ICCV 2015. </li>-->
    </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Publications 
[<a href="http://scholar.google.com/citations?user=yYviZ-oAAAAJ&hl=en">Google Scholar</a>]</h2>


<div class="paper" id="IJCAI2018"><img class="paper" 
src="./Imgs_files/DLDL-v2-fw.png"
title="Age Estimation Using Expectation of Label Distribution Learning">
<div> <strong>Age Estimation Using Expectation of Label Distribution Learning</strong><br>
<strong>Bin-Bin Gao</strong>, Hong-Yu Zhou, Jianxin Wu and Xin Geng<br>
<i>In: Proceedings of the 27th International Joint Conference on Artificial Intelligence (<strong>IJCAI 2018</strong>), Stockholm, Sweden, July 2018. </i>(<strong>accepted.</strong>)<br>
[ <a href="./Pub_files/IJCAI2018_DLDLv2.pdf">Paper</a>  ] 
[ <a href="./Projects/DLDL-v2.html">Project Page</a>  ]
[ <a href="javascript:toggleabs('IJCAI2018')">Abstrcat</a> ]
<preabs style="display: none;">
Age estimation performance has been greatly improved by using convolutional neural network. However, existing methods have an inconsistency between the training objectives and evaluation metric, so they may be suboptimal. In addition, these methods always adopt image classification or face recognition models with a large amount of parameters which bring expensive computation cost and storage overhead. To alleviate these issues, we design a light network architecture and propose a unified framework which can jointly learn age distribution and regress age. The effectiveness of our approach has been demonstrated on apparent and real age estimation tasks. Our method achieves new state-of-the-art results using the single model with 36$\times$ fewer parameters and 2.6$\times$ reduction in inference time. Moreover, our method can achieve comparable results as the state-of-the-art even though model parameters are further reduced to 0.9M~(3.8MB disk storage). We also analyze that Ranking methods are implicitly learning label distributions.
</preabs>      
[ <a href="javascript:togglebib('IJCAI2018')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{gaoDLDLv2,
           title={Age Estimation Using Expectation of Label Distribution Learning},
           author={Gao, Bin-Bin and Zhou, Hong-Yu and Wu, Jianxin and Geng, Xin},
           booktitle={Proceedings of the 27th International Joint Conference on Artificial Intelligence (<strong>IJCAI 2018</strong>)},
           pages={xx--xx},
           year={2018}
            }

</pre>
[<a href="http://www.ccf.org.cn/xspj/rgzn/">CCF-A</a>]
<br>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="SSI2018">
<a href="http://engine.scichina.com/publisher/scp/journal/SSI/doi/10.1360/N112017-00216?slug=abstract">
<img class="paper"
 src="./Imgs_files/SSI2018.png"
 title="Resource constrained deep learning: Challenges and Practices">
</a>
<div> <strong>Resource Constrained Deep Learning: Challenges and Practices (in Chinese)</strong><br>
Jianxin Wu, <strong>Bin-Bin Gao</strong>, Xiu-Shen Wei and Jian-Hao Luo<br>
<i>SCIENTIA SINICA Informatics, 48(5):501-510,2018. </i><br>
[ <a href="http://engine.scichina.com/publisher/scp/journal/SSI/doi/10.1360/N112017-00216?slug=abstract">Paper</a>  ] 
[ <a href="javascript:toggleabs('SSI2018')">Abstrcat</a> ]
<preabs style="display: none;">
Deep learning has made significant progresses in recent years. However, deep models require a lot of
computation-related resources, and its learning process needs huge number of data points and their labels. Hence,
one current research focus in deep learning is to reduce its resource consumptions, i.e., resource constrained deep
learning. In this paper, we first analyze deep learning’s thirsts for the various types of resources and the challenges
they lead to, then briefly introduce research progresses from three aspects: data, label and computation resources.
And we give detailed introductions of these areas using our research results as examples.
</preabs>      
[ <a href="javascript:togglebib('SSI2018')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{SSI2018,
           title={Resource constrained deep learning: Challenges and practices},
           author={Wu, Jianxin and Gao, Bin-Bin and Wei, Xiu-Shen and Luo, Jian-Hao},
           journal={SCIENTIA SINICA Informatics},
           volume={48},
           number={5},
           pages={501--510},
           year={2018}
            }
</pre>
<br>
</div>
<div class="spanner"></div>
</div>




<div class="paper" id="ICCV17">
<img class="paper" 
src="./Imgs_files/1AF_ICCV2017.png" 
title="Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors">
<div> <strong>Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors</strong><br>
Hong-Yu Zhou, <strong>Bin-Bin Gao</strong> and Jianxin Wu <br>
<i>In: In: Proceedings of the IEEE International Conference on Computer Vision </i>(<strong>ICCV 2017</strong>), Venice, Italy, October 2017, pp. 3505-3513.<br>
[ <a href="./Pub_files/AF_ICCV2017.pdf">Paper</a>  ] 
[ <a href="https://funnyzhou.github.io/project/adaptive_feeding/">Project Page</a>  ]
[ <a href="javascript:toggleabs('ICCV17')">Abstrcat</a> ]
<preabs style="display: none;">
Object detection aims at high speed and accuracy simultaneously. However, fast models are usually less accurate, while accurate models cannot satisfy our need for speed. A fast model can be 10 times faster but 50% less accurate than an accurate model. In this paper, we propose Adaptive Feeding (AF) to combine a fast (but less accurate) detector and an accurate (but slow) detector, by adaptively determining whether an image is easy or hard and choosing an appropriate detector for it. In practice, we build a cascade of detectors, including the AF classifier which make the easy vs. hard decision and the two detectors. The AF classifier can be tuned to obtain different tradeoff between speed and accuracy, which has negligible training time and requires no additional training data. Experimental results on the PASCAL VOC, MS COCO and Caltech Pedestrian datasets confirm that AF has the ability to achieve comparable speed as the fast detector and comparable accuracy as the accurate one at the same time. As an example, by combining the fast SSD300 with the accurate SSD500 detector, AF leads to 50% speedup over SSD500 with the same precision on the VOC2007 test set.
</preabs>      
[ <a href="javascript:togglebib('ICCV17')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{zhou2017adaptive,
           title={Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors},
           author={Zhou, Hong-Yu and Gao, Bin-Bin and Wu, Jianxin},
           booktitle={Proceedings of the IEEE International Conference on Computer Vision (ICCV 2017)},
           pages={3505-3513},
           year={2017}
            }
</pre>
[<a href="http://www.ccf.org.cn/xspj/rgzn/">CCF-A</a>]
<br>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="BMVC17">
<img class="paper" 
src="./Imgs_files/BMVC2017_SoS.png" 
title="Sunrise or Sunset: Selective Comparison Learning for Subtle Attribute Recognition">
<div> <strong>Sunrise or Sunset: Selective Comparison Learning for Subtle Attribute Recognition</strong><br>
Hong-Yu Zhou, <strong>Bin-Bin Gao</strong> and Jianxin Wu <br>
<i>In:In: Proceedings of the 28th British Machine Vision Conference </i>(<strong>BMVC 2017</strong>), London, UK, September 2017.<br>
[ <a href="./Pub_files/BMVC2017_SoS.pdf">Paper</a>  ] 
[ <a href="https://funnyzhou.github.io/project/sos/">Project Page</a>  ]
[ <a href="javascript:toggleabs('BMVC17')">Abstrcat</a> ]
<preabs style="display: none;">
The difficulty of image recognition has gradually increased from general category recognition to fine-grained recognition and to the recognition of some subtle attributes such as temperature and geolocation. In this paper, we try to focus on the classification between sunrise and sunset and hope to give a hint about how to tell the difference in subtle attributes. Sunrise vs. sunset is a difficult recognition task, which is challenging even for humans. Towards understanding this new problem, we first collect a new dataset made up of over one hundred webcams from different places. Since existing algorithmic methods have poor accuracy, we propose a new pairwise learning strategy to learn features from selective pairs of images. Experiments show that our approach surpasses baseline methods by a large margin and achieves better results even compared with humans. We also apply our approach to existing subtle attribute recognition problems, such as temperature estimation, and achieve state-of-the-art results.
</preabs>      
[ <a href="javascript:togglebib('BMVC17')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{zhou2017sunrise,
            title={Sunrise or Sunset: Selective Comparison Learning for Subtle Attribute Recognition},
            author={Zhou, Hong-Yu and Gao, Bin-Bin and Wu, Jianxin},
            booktitle={Proceedings of the 28th British Machine Vision Conference (BMVC 2017)},
            year={2017}
            }
</pre>
[<a href="http://www.ccf.org.cn/xspj/rgzn/">CCF-C</a>]
<br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="TIP17">
<img class="paper" 
src="./Imgs_files/DLDL_LD.png" 
title="Deep Label Distribution Learning with Label Ambiguity ">
<div> <strong>Deep Label Distribution Learning with Label Ambiguity</strong><br>
<strong>Bin-Bin Gao</strong>, Chao Xing, Chen-Wei Xie, Jianxin Wu and Xin Geng <br>
<i>IEEE Transactions on Image Processing </i>(<strong>TIP 2017</strong>), 26(6):2825-2838,2017.<br>
[ <a href="./Pub_files/TIP2017_DLDL.pdf">Paper</a>  ] 
[ <a href="./Projects/DLDL.html">Project Page</a>  ]
[ <a href="javascript:toggleabs('TIP17')">Abstrcat</a> ]
<preabs style="display: none;">
Convolutional Neural Networks (ConvNets) have achieved excellent recognition performance in various visual recognition tasks. A large labeled training set is one of the most important factors for its success. However, it is difficult to collect sufficient training images with precise labels in some domains such as apparent age estimation, head pose estimation, multi-label classification and semantic segmentation. Fortunately, there is ambiguous information among labels, which makes these tasks different from traditional classification. Based on this observation, we convert the label of each image into a discrete label distribution, and learn the label distribution by minimizing a Kullback-Leibler divergence between the predicted and ground-truth label distributions using deep ConvNets. The proposed DLDL (Deep Label Distribution Learning) method effectively utilizes the label ambiguity in both feature learning and classifier learning, which prevents the network from over-fitting even when the training set is small. Experimental results show that the proposed approach produces significantly better results than state-of-the-art methods for age estimation and head pose estimation. At the same time, it also improves recognition performance for multi-label classification and semantic segmentation tasks.
</preabs>      
[ <a href="javascript:togglebib('TIP17')">BibTeX</a> ]
<pre style="display: none;">@ARTICLE{gao2016deep,
         author={Gao, Bin-Bin and Xing, Chao and Xie, Chen-Wei and Wu, Jianxin and Geng, Xin},
         title={Deep Label Distribution Learning with Label Ambiguity},
         journal={IEEE Transactions on Image Processing},
         year={2017},
         volume={26},
         number={6},
         pages={2825-2838}, 
         }
</pre>
[<a href="./Pub_files/TIPJCR.pdf">JCR-Q1</a>, <a href="http://www.ccf.org.cn/xspj/jsjtxxydmt/">CCF-A</a>]
<br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="CVPR16">
<img class="paper" 
src="./Imgs_files/Fev_Lev.png" 
title="Exploit Bounding Box Annotations for Multi-label Object Recognition">
<div> <strong>Exploit Bounding Box Annotations for Multi-label Object Recognition</strong><br>
Hao Yang, Joey Tiany Zhou, Yu Zhang, <strong>Bin-Bin Gao</strong>, Jianxin Wu and Jianfei Cai<br>
<i>In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition </i>(<strong>CVPR 2016</strong>), Las Vegas, NV, USA, June 2016, pp.280-288.<br>
[ <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Exploit_Bounding_Box_CVPR_2016_paper.pdf">Paper</a>  ]
[ <a href="javascript:toggleabs('CVPR16')">Abstrcat</a> ]
<preabs style="display: none;">
Convolutional neural networks (CNNs) have shown
great performance as general feature representations for
object recognition applications. However, for multi-label
images that contain multiple objects from different categories,
scales and locations, global CNN features are not
optimal. In this paper, we incorporate local information
to enhance the feature discriminative power. In particular,
we first extract object proposals from each image. With
each image treated as a bag and object proposals extracted
from it treated as instances, we transform the multi-label
recognition problem into a multi-class multi-instance learning
problem. Then, in addition to extracting the typical
CNN feature representation from each proposal, we propose
to make use of ground-truth bounding box annotations
(strong labels) to add another level of local information
by using nearest-neighbor relationships of local regions to
form a multi-view pipeline. The proposed multi-view multiinstance
framework utilizes both weak and strong labels
effectively, and more importantly it has the generalization
ability to even boost the performance of unseen categories
by partial strong labels from other categories. Our framework
is extensively compared with state-of-the-art handcrafted
feature based methods and CNN based methods on
two multi-label benchmark datasets. The experimental results
validate the discriminative power and the generalization
ability of the proposed framework. With strong labels,
our framework is able to achieve state-of-the-art results in
both datasets.
</preabs>
[ <a href="javascript:togglebib('CVPR16')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{yang2016exploit,
  title={Exploit bounding box annotations for multi-label object recognition},
  author={Yang, Hao and Zhou, Joey Tianyi and Zhang, Yu and Gao, Bin-Bin and Wu, Jianxin and Cai, Jianfei},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={280--288},
  year={2016}
}
</pre>
[<a href="http://www.ccf.org.cn/xspj/rgzn/">CCF-A</a>]
<br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="AAAI2016">
<img class="paper" 
src="./Imgs_files/D3.png" 
title="Representing Sets of Instances for Visual Recognition">
<div> <strong>Representing Sets of Instances for Visual Recognition</strong><br>
Jianxin Wu, <strong>Bin-Bin Gao</strong> and Guoqing Liu <br>
<i>In: Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence </i>(<strong>AAAI 2016</strong>), Phoenix, Arizona, USA, Feb 2016, pp.2237-2243.<br>
[ <a href="http://cs.nju.edu.cn/_upload/tpl/00/ed/237/template237/paper/AAAI2016_D3.pdf">Paper</a>  ] 
[ <a href="javascript:toggleabs('AAAI2016')">Abstrcat</a> ]
<preabs style="display: none;">
In computer vision, a complex entity such as an image or
video is often represented as a set of instance vectors, which
are extracted from different parts of that entity. Thus, it is
essential to design a representation to encode information
in a set of instances robustly. Existing methods such as FV
and VLAD are designed based on a generative perspective,
and their performances fluctuate when difference types of instance
vectors are used (i.e., they are not robust). The proposed
D3 method effectively compares two sets as two distributions,
and proposes a directional total variation distance
(DTVD) to measure their dissimilarity. Furthermore, a robust
classifier-based method is proposed to estimate DTVD robustly,
and to efficiently represent these sets. D3 is evaluated
in action and image recognition tasks. It achieves excellent
robustness, accuracy and speed.
</preabs>
[ <a href="javascript:togglebib('AAAI2016')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{wu2016representing,
  title={Representing Sets of Instances for Visual Recognition.},
  author={Wu, Jianxin and Gao, Bin-Bin and Liu, Guoqing},
  booktitle={Proceedings of the theritieth AAAI Conference on Artificial Intelligence},
  pages={2237--2243},
  year={2016}
}
</pre>
[<a href="http://www.ccf.org.cn/xspj/rgzn/">CCF-A</a>]
<br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ICCV15_AGE">
<img class="paper" 
src="./Imgs_files/ICCV15_AGE.png" 
title="Deep Label Distribution Learning for Apparent Age Estimation ">
<div> <strong>Deep Label Distribution Learning for Apparent Age Estimation </strong><br>
Xu Yang, <strong>Bin-Bin Gao</strong>, Chao Xing, Zeng-Wei Huo, Xiu-Shen Wei, Ying Zhou, Jianxin Wu and Xin Geng <br>
<i>In: Proceedings of the IEEE ICCV’15 ChaLearn Looking at People workshop </i>(<strong>ICCVW 2015</strong>), Santiago, Chile, Dec 2015, pp.102-108.<br>
[ <a href="./Pub_files/iccvw15_AGE.pdf">Paper</a>  ]  
[ <a href="./Pub_files/iccvw15_AGE_slide.pdf">Slides</a> ] 
[ <a href="javascript:toggleabs('ICCV15_AGE')">Abstrcat</a> ]
<preabs style="display: none;">
In the age estimation competition organized by
ChaLearn, apparent ages of images are provided. Uncertainty
of each apparent age is induced because each image
is labeled by multiple individuals. Such uncertainty makes
this age estimation task different from common chronological
age estimation tasks. In this paper, we propose a
method using deep CNN (Convolutional Neural Network)
with distribution-based loss functions. Using distributions
as the training tasks can exploit the uncertainty induced by
manual labeling to learn a better model than using ages as
the target. To the best of our knowledge, this is one of the
first attempts to use the distribution as the target of deep
learning. In our method, two kinds of deep CNN models are
built with different architectures. After pre-training each
deep CNN model with different datasets as one corresponding
stream, the competition dataset is then used to fine-tune
both deep CNN models. Moreover, we fuse the results of
two streams as the final predicted ages. In the final testing
dataset provided by competition, the age estimation performance
of our method is 0.3057, which is significantly better
than the human-level performance (0.34) provided by the
competition organizers.
</preabs>
[ <a href="javascript:togglebib('ICCV15_AGE')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{yang2015deep,
  title={Deep label distribution learning for apparent age estimation},
  author={Yang, Xu and Gao, Bin-Bin and Xing, Chao and Huo, Zeng-Wei and Wei, Xiu-Shen and Zhou, Ying and Wu, Jianxin and Geng, Xin},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision Workshops},
  pages={102--108},
  year={2015}
}
</pre>
<br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ICCV15_CER">
<img class="paper" 
src="./Imgs_files/ICCV15_CER.png" 
title="Deep Spatial Pyramid Ensemble for Cultural Event  Recognition">
<div> <strong>Deep Spatial Pyramid Ensemble for Cultural Event  Recognition</strong><br>
Xiu-Shen Wei, <strong>Bin-Bin Gao</strong> and Jianxin Wu<br>
<i>In: Proceedings of the IEEE ICCV’15 ChaLearn Looking at People workshop </i>(<strong>ICCVW 2015</strong>), Santiago, Chile, Dec 2015, pp.38-44.<br>
[ <a href="./Pub_files/iccvw15_CER.pdf">Paper</a>  ]  
[ <a href="./Pub_files/iccvw15_CER_slide.pdf">Slides</a> ] 
[ <a href="javascript:toggleabs('ICCV15_CER')">Abstrcat</a> ]
<preabs style="display: none;">
Semantic event recognition based only on image-based
cues is a challenging problem in computer vision. In order
to capture rich information and exploit important cues
like human poses, human garments and scene categories,
we propose the Deep Spatial Pyramid Ensemble framework,
which is mainly based on our previous work, i.e., Deep Spatial
Pyramid (DSP). DSP could build universal and powerful
image representations from CNN models. Specifically,
we employ five deep networks trained on different data
sources to extract five corresponding DSP representations
for event recognition images. For combining the complementary
information from different DSP representations, we
ensemble these features by both “early fusion” and “late
fusion”. Finally, based on the proposed framework, we
come up with a solution for the track of the Cultural Event
Recognition competition at the ChaLearn Looking at People
(LAP) challenge in association with ICCV 2015. Our
framework achieved one of the best cultural event recognition
performance in this challenge.
</preabs>
[ <a href="javascript:togglebib('ICCV15_CER')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{wei2015deep,
  title={Deep spatial pyramid ensemble for cultural event recognition},
  author={Wei, Xiu-Shen and Gao, Bin-Bin and Wu, Jianxin},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision Workshops},
  pages={38--44},
  year={2015}
}
</pre>
<br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ICMLA15">
<img class="paper" 
src="./Imgs_files/ICMLA15.png" 
title="Coordinate Descent Fuzzy Twin Support Vector Machine for Classification">
<div> <strong>Coordinate Descent Fuzzy Twin Support Vector Machine for Classification</strong><br>
<strong>Bin-Bin Gao</strong>, Jian-Jun Wang, Yao Wang and Chan-Yun Yang<br>
<i>In: Proceedings of the IEEE Conference on Machine Learning and Applications </i>(<strong>ICMLA 2015</strong>), Miami, Florida, USA, Dec 2015, pp.7-12.<br>
[ <a href="./Pub_files/icmla15_FTWSVM.pdf">Paper</a>  ] 
[ <a href="https://github.com/gaobb/CDFTSVM">Code</a> ]
[ <a href="javascript:toggleabs('ICMLA15')">Abstrcat</a> ]
<preabs style="display: none;">
In this paper, we develop a novel coordinate descent
fuzzy twin SVM (CDFTSVM) for classification. The proposed
CDFTSVM not only inherits the advantages of twin SVM but
also leads to a rapid and robust classification results. Specifically,
our CDFTSVM has two distinguished advantages: (1) An effective
fuzzy membership function is produced for removing the noise
incurred by the contaminant inputs. (2) A coordinate descent
strategy with shrinking by active set is used to deal with the
computational complexity brought by the high dimensional input.
In addition, a series of simulation experiments are conducted to
verify the performance of the CDFTSVM, which further supports
our previous claims.
</preabs>
[ <a href="javascript:togglebib('ICMLA15')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{gao2015coordinate,
  title={Coordinate Descent Fuzzy Twin Support Vector Machine for Classification},
  author={Gao, Bin-Bin and Wang, Jian-Jun and Wang, Yao and Yang, Chan-Yun},
  booktitle={Proceedings of the IEEE Conference on Machine Learning and Applications (ICMLA)},
  pages={7-12},
  year={2015},
}
</pre>
<br>
</div>
<div class="spanner"></div>
</div>
<!--
<div class="paper" id="KMTSVM">
<img class="paper" 
src="./Imgs_files/KMTSVM.png" 
title="Maximum Margin Twin Support Vector Machine for Multi-Class Classification">
<div> <strong>Maximum Margin Twin Support Vector Machine for Multi-Class Classification (in Chinese)</strong><br>
<strong>Bin-Bin Gao</strong> and Jian-Jun Wang<br>
<i>Journal of Southwest China Normal University (Natural Science Edition)</i>,10(38):130-135,2013..<br>
[ <a href="./Pub_files/KMTSVM.pdf">Paper</a>  ] 
[ <a href="javascript:toggleabs('KMTSVM')">Abstrcat</a> ]
<preabs style="display: none;">
A novel maximum margin twin support vector machine for multi-class classification (K-MTSVM) is presented in this paper. 
The K-MTSVM takes structural risk minimization principle as the optimization objective to build classification model by
introducing the margin and uses a 1-versus-1-versus-rest structure to train sub-classifiers. The experimental results on 
both artificial and UCI datasets indicate that our K-MTSVM gets better generalization performance.
</preabs>
[ <a href="javascript:togglebib('KMTSVM')">BibTeX</a> ]
<pre style="display: none;">
</pre>
<br>
</div>
<div class="spanner"></div>
</div>
-->

 
</div>
</div>


<div class="section">
<h2 id="confpapers">Technical Reports</h2>

<div class="paper" id="DSP">
<img class="paper" 
src="./Imgs_files/DSP.png" 
title="Deep Spatial Pyramid: The Devil is Once Again in the Details">
<div> <strong>Deep Spatial Pyramid: The Devil is Once Again in the Details</strong><br>
<strong>Bin-Bin Gao</strong>, Xiu-Shen Wei, Jianxin Wu, Weiyao Lin<br>
<i>arXiv:1504.05277v2, 2015. </i><br>
[ <a href="https://arxiv.org/pdf/1504.05277v2.pdf">Paper</a>  ] 
[ <a href="https://github.com/gaobb/DSP">Code</a> ]
[ <a href="javascript:toggleabs('DSP')">Abstrcat</a> ]
<preabs style="display: none;">
In this paper we show that by carefully making good
choices for various detailed but important factors in a visual
recognition framework using deep learning features,
one can achieve a simple, efficient, yet highly accurate image
classification system. We first list 5 important factors,
based on both existing researches and ideas proposed in this
paper. These important detailed factors include: 1) `2 matrix
normalization is more effective than unnormalized or `2
vector normalization, 2) the proposed natural deep spatial
pyramid is very effective, and 3) a very small K in Fisher
Vectors surprisingly achieves higher accuracy than normally
used large K values. Along with other choices (convolutional
activations and multiple scales), the proposed
DSP framework is not only intuitive and efficient, but also
achieves excellent classification accuracy on many benchmark
datasets. For example, DSP’s accuracy on SUN397 is
59.78%, significantly higher than previous state-of-the-art
(53.86%).
</preabs>
[ <a href="javascript:togglebib('DSP')">BibTeX</a> ]
<pre style="display: none;">@article{GaoDSP15,
  author    = {Gao, Bin-Bin and Wei, Xiu-Shen and Wu, Jianxin and Lin Weiyao},
  title     = {Deep Spatial Pyramid: The Devil is Once Again in the Details},
  journal   = {CoRR},
  volume    = {abs/1504.05277},
  year      = {2015},
  url       = {http://arxiv.org/abs/1504.05277},
}
</pre>
<br>
</div>
<div class="spanner"></div>
</div>
</div>
</div>


<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Awards</h2>
<div class="paper">
<ul>
<li><strong>Nanruijibao Scholarship</a></strong> in Nanjing University, 2016.</li>
<li><strong>Second-class Academic Scholarship</a></strong> of Nanjing University, 2014-2015 & 2015-2016.</li>
<li><strong>Outstanding Thesis Award</a></strong> of Southwest University, 2013.</li>
<li><strong>First-class Academic Scholarship</a></strong> of Southwest University, 2011-2012.</li>
<li><strong>Outstanding Undergraduates Awards</a></strong>, 2010.</li>
<li><strong>National Scholarship for Encouragement</a></strong>, 2007-2008 & 2008-2009.</li>
</ul>
<div class="spanner"></div>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Competitions</h2>
<div class="paper">
<ul>
<li><strong>First runner-up</a></strong> in Cultural Event Recognition at ICCV 2015.(with Xiu-Shen Wei and Jianxin Wu)</li>
<li><strong>Fourth place</a></strong> in Apprament Age Estimation at ICCV 2015. </li> 
<li><strong>Meritorious Winner</a></strong> of Certificate Authority Cup Mathematical Contest in Modeling, 2012.(with Qiu-Lin Li and Hong-Yan Yang)</li>
<li><strong>Second Prize</a></strong> in China Graduate Mathematical Contest in Modeling (CGMCM), 2011.(with Qiu-Lin Li and Ji-Lian Guo)</li>
<li><strong>Third Prize</a></strong> in China Undergraduate Mathematical Contest (Mathematics, Finals) (CMC), 2010.</li>
<li><strong>First Prize</a></strong> in China Undergraduate Mathematical Contest (Mathematics, Preliminaries) (CMC), 2009.</li>
</ul>
<div class="spanner"></div>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Teaching Assistants</h2>
<div class="paper">
<ul>
<li><strong><a href="https://cs.nju.edu.cn/wujx/teaching_PR.html">Pattern Recognition</a></strong> (<small>for undergraduate and graduated students. Spring, 2017.</small>)</li>
<ol> 
<li><P> &nbsp; <a href="http://cs.nju.edu.cn/wujx/teaching_PR.html">Course Page</a></P></li>
<li><P> &nbsp; <a href="./teaching.html">Assignments Page</a></P></li>
</ol>
<li><strong><a href="https://cs.nju.edu.cn/tb/prob.htm">Probability and Statistics</a></strong> (<small>for undergraduate students. Spring, 2017.</small>)</li>
<ol> 
<li><P> &nbsp; <a href="https://cs.nju.edu.cn/tb/prob.htm">Course Page</a></P></li>
<li><P> &nbsp; <a href="./teaching.html">Assignments Page</a></P></li>
</ul>
<div class="spanner"></div>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Professional Activities</h2>
<div class="paper">
<ul>
<p><font size="5">Conference Reviewer: <a href="https://aaai.org/Conferences/AAAI-19/">AAAI 2019</a>, <a http://accv2018.net">ACCV 2018</a>, <a href="https://eccv2018.org/">ECCV 2018</a>, <a href="http://cvpr2018.thecvf.com/">CVPR 2018</a>, <a href="http://cvpr2017.thecvf.com/">CVPR 2017</a>, <a href="http://iccv2017.thecvf.com/">ICCV 2017</a>, <a href="http://www.aaai.org/Press/Proceedings/aaai16.php">AAAI 2016</a></font></p>
<p><font size="5">Journal Reviewer: <a href="https://www.journals.elsevier.com/neural-networks/">Elsevier Journal of Neural Networks (NN)</a>, <a href="	http://cis.ieee.org/ieee-transactions-on-neural-networks-and-learning-systems.html">IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</a></font></p>
</ul>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Correspondence</h2>
<div class="paper">
<ul>
<p><font size="5"><strong>Email:<a href="mailto:csgaobb@gmail.com"></strong> csgaobb@gmail.com</a></font></p>
<p><font size="5">Bin-Bin Gao</font></p>
<p><font size="5">National Key Laboratory for Novel Software Technology</font></p>
<p><font size="5">Nanjing University</font></p>
<p><font size="5">Nanjing 210023, China</font></p>
<p><font size="5">913, Laboratory: Computer Science Building, Xianlin Campus of Nanjing University</font></p>
<p><font size="5"><a href="http://lamda.nju.edu.cn/gaobb/"></strong>Lamda homepage</a></font></p>
<p><font size="5"><a href="http://csgaobb.github.io"></strong>Github homepage</a></font></p>
</ul>
</div>
</div>
</div>
<hr>
<div style="clear: both;">
<div class="section">
<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC8zMTM0MC83OTAz">
<script type="text/javascript">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
</script>
<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->
</div>
</div>

<div style="clear:both;">
<!--<p align="right"><font size="2"><a href="http://lamda.nju.edu.cn/gaobb">Updated on Jun.  6, 2017.</a></font></p>-->
<p align="right"><font size="2"><a href="http://lamda.nju.edu.cn/gaobb">Updated on May 21, 2018.</a></font></p>
</div>

<hr>
<div id="google_translate_element"></div>
<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({pageLanguage: 'en', layout: google.translate.TranslateElement.InlineLayout.SIMPLE}, 'google_translate_element');
}
</script><script type="text/javascript" src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script>


<div style="clear: both;">
<div class="smallsection">
     <!-- lamda.nju.edu.cn/gaobb-->
     <!--<script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=SPmghZAygzRWO5bn8cZHpTvhY_dkyj3w_HGM3_T4EfQ"></script>-->
     <!-- csgaobb.github.io-->
     <script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=lFQx-PXyHBymj3FqoTTJWIaTiAap63Cpqt0uNyBYmBU"></script>
</div>
</div>


</body>
</html>
