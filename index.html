<!-- saved from url=(0031)https://csgaobb.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
<title>Bin-Bin Gao's homepage</title>
<link rel="shortcut icon" href="./Imgs_files/binbin_favicon.ico">
<meta content="Bin-Bin Gao, csgaobb.github.io, 高斌斌" name="keywords">
<style media="screen" type="text/css"></style>

<link href="./main_files/homepage.css" rel="stylesheet" type="text/css">
<!--<link rel="stylesheet" href="./main_files/owl.carousel.css">
<link rel="stylesheet" href="./main_files/owl.theme.css">-->

<!-- Google Analytics -->
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-69879514-1']);
    _gaq.push(['_trackPageview']);

    (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>

</script>
<script src='./main_files/hidebib.js' type="text/javascript"></script>
</head>


<!-- Google custom search -->
  <script type="text/javascript">
    (function() {
    var cx = '015564147751910566765:wsger_p3dsg';
    var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
    '//www.google.com/cse/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
    })();
  </script>


<body>
<div class="searchbox">
     <gcse:searchbox-only resultsUrl="./search.html"
           autoCompleteMaxCompletions="5"
           autoCompleteMatchType="any"/>
</div>
<div style="margin-bottom: 0em; border: 0px solid #ddd; background-color: #fff; padding: 1em; height: 185px;">
<div style="margin: 0px auto; width: 100%;">
<img title="Bin-Bin Gao" style="float: left; padding-left: 2.5em; padding-right: 2.5em; height: 180px;" src="./Imgs_files/gaobb2.jpg">
<div style="padding-left: 2em; padding-right: 2em;vertical-align: top; height: 180px;"><span style="line-height: 150%; font-size: 20pt;">Bin-Bin Gao</span><br>
<span>Ph.D. candidate</span> <br>
<span><a href="http://lamda.nju.edu.cn/">LAMDA Group</a></span> <br>
<span><a href="https://keysoftlab.nju.edu.cn/">National Key Laboratory for Novel Software Technology</a></span> <br>
<span><a href="http://cs.nju.edu.cn/">Department of Computer Science &amp; Technology</a></span> <br>
<span><a href="http://nju.edu.cn/">Nanjing University</a></span> <br>
<span>Supervisor:<a href="http://cs.nju.edu.cn/wujx">Prof. Jianxin Wu</a></span> <br><br>

<span>Nanjing 210023, China</span><br>
<span><strong>Email: <a href="mailto:csgaobb@gmail.com"></strong>csgaobb@gmail.com</a>   or <a href="mailto:gaob@lamda.nju.edu.cn"></strong>gaobb@lamda.nju.edu.cn</a></span> <br><br>


</div>
</div>
</div>
<hr>


<div style="clear: both;">
<div class="section">
<h2>Biography
[<a href="./main_files/GAOBB_CV.pdf">CV</a>]</h2>

<div style="text-align:justify;">
<div class="paper" >
Currently I am a third year PH.D. student of <a href="http://cs.nju.edu.cn/" target="_blank">Department of Computer Science and Technology</a> 
in <a href="http://www.nju.edu.cn/" target="_blank">Nanjing University</a> and a member of <a href="http://lamda.nju.edu.cn/" target="_blank">LAMDA Group</a>, 
led by <a href="http://lamda.nju.edu.cn/zhouzh/" target="_blank">Prof. Zhi-Hua   Zhou.</a><br>
<br>
I received my M.Sc. degree in applied mathematics in June 2013 from <a href="http://swu.edu.cn/">Southwest University</a>, 
Chongqing China. Since september 2014, I was admitted to study for a Ph.D. degree in <a href="http://www.nju.edu.cn/" target="_blank">Nanjing University</a>.
</div>
</div>
</div>
</div>


<div style="clear: both;">
<div class="section">
  <h2>News</h2>
  <div class="paper">
    <ul>
    <li> Mar. 26, 2017: A journal paper on deep label distribution learning is accepted by <a href="http://signalprocessingsociety.org/publications-resources/ieee-transactions-image-processing"> IEEE Trans. Image Processing </a>.</li>
    <li> Dec. 11-17, 2015: Attended <a href="http://pamitc.org/iccv15/"> ICCV 2015</a>.</li>
    <li> Nov. 13, 2015: A paper accepted for <a href="http://aaai.org/"> AAAI 2016</a>. </li>
    <li> Oct. 10, 2015: Two papers accepted for ICCV 2015 Workshop. </li>
    <li> Sep. 20, 2015: First runner-up in <a href="http://gesture.chalearn.org/"> Cultural Event Recognition </a> at ICCV 2015. (with X.-S Wei and J. Wu)</li>
    <li> Sep. 20, 2015: The fourth place in <a href="http://gesture.chalearn.org/"> Apprament Age Estimation </a> at ICCV 2015. </li>
    </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Publications 
[<a href="http://scholar.google.com/citations?user=yYviZ-oAAAAJ&hl=en">Google Scholar</a>]</h2>


<div class="paper" id="TIP17"><img class="paper" 
src="./Imgs_files/DLDL_LD.png" 
title="Deep Label Distribution Learning with Label Ambiguity ">
<div> <strong>Deep Label Distribution Learning with Label Ambiguity</strong><br>
<i><strong>Bin-Bin Gao</strong>, Chao Xing, Chen-Wei Xie, Jianxin Wu and Xin Geng</i> <br>
IEEE Transactions on Image Processing (<strong>TIP</strong>), 2017.<br>
[ <a href="./Pub_files/TIP2017_DLDL.pdf">Paper</a>  ] 
[ <a href="./Projects/DLDL.html">Project Page</a>  ]
[ <a href="javascript:toggleabs('TIP17')">Abstrcat</a> ]
<preabs style="display: none;">
Convolutional Neural Networks (ConvNets) have achieved excellent recognition performance in various visual recognition tasks. A large labeled training set is one of the most important factors for its success. However, it is difficult to collect sufficient training images with precise labels in some domains such as apparent age estimation, head pose estimation, multi-label classification and semantic segmentation. Fortunately, there is ambiguous information among labels, which makes these tasks different from traditional classification. Based on this observation, we convert the label of each image into a discrete label distribution, and learn the label distribution by minimizing a Kullback-Leibler divergence between the predicted and ground-truth label distributions using deep ConvNets. The proposed DLDL (Deep Label Distribution Learning) method effectively utilizes the label ambiguity in both feature learning and classifier learning, which prevents the network from over-fitting even when the training set is small. Experimental results show that the proposed approach produces significantly better results than state-of-the-art methods for age estimation and head pose estimation. At the same time, it also improves recognition performance for multi-label classification and semantic segmentation tasks.
</preabs>      
[ <a href="javascript:togglebib('TIP17')">BibTeX</a> ]
<pre style="display: none;">@ARTICLE{gao2016deep,
         author={Gao, Bin-Bin and Xing, Chao and Xie, Chen-Wei and Wu, Jianxin and Geng, Xin},
         title={Deep Label Distribution Learning with Label Ambiguity},
         journal={IEEE Transactions on Image Processing},
         year={2017},
         volume={26},
         number={6},
         pages={2825-2838}, 
         }
</pre>
<br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="CVPR16"><img class="paper" 
src="./Imgs_files/Fev_Lev.png" 
title="Exploit Bounding Box Annotations for Multi-label Object Recognition">
<div> <strong>Exploit Bounding Box Annotations for Multi-label Object Recognition</strong><br>
<i>Hao Yang, Joey Tiany Zhou, Yu Zhang, <strong>Bin-Bin Gao</strong>, Jianxin Wu and Jianfei Cai</i><br>
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), Las Vegas, NV, USA, 2016.<br>
[ <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Exploit_Bounding_Box_CVPR_2016_paper.pdf">Paper</a>  ]
[ <a href="javascript:toggleabs('CVPR16')">Abstrcat</a> ]
<preabs style="display: none;">
Convolutional neural networks (CNNs) have shown
great performance as general feature representations for
object recognition applications. However, for multi-label
images that contain multiple objects from different categories,
scales and locations, global CNN features are not
optimal. In this paper, we incorporate local information
to enhance the feature discriminative power. In particular,
we first extract object proposals from each image. With
each image treated as a bag and object proposals extracted
from it treated as instances, we transform the multi-label
recognition problem into a multi-class multi-instance learning
problem. Then, in addition to extracting the typical
CNN feature representation from each proposal, we propose
to make use of ground-truth bounding box annotations
(strong labels) to add another level of local information
by using nearest-neighbor relationships of local regions to
form a multi-view pipeline. The proposed multi-view multiinstance
framework utilizes both weak and strong labels
effectively, and more importantly it has the generalization
ability to even boost the performance of unseen categories
by partial strong labels from other categories. Our framework
is extensively compared with state-of-the-art handcrafted
feature based methods and CNN based methods on
two multi-label benchmark datasets. The experimental results
validate the discriminative power and the generalization
ability of the proposed framework. With strong labels,
our framework is able to achieve state-of-the-art results in
both datasets.
</preabs>
[ <a href="javascript:togglebib('CVPR16')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{yang2016exploit,
  title={Exploit bounding box annotations for multi-label object recognition},
  author={Yang, Hao and Zhou, Joey Tianyi and Zhang, Yu and Gao, Bin-Bin and Wu, Jianxin and Cai, Jianfei},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={280--288},
  year={2016}
}
</pre>
<br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="AAAI2016"><img class="paper" 
src="./Imgs_files/D3.png" 
title="Representing Sets of Instances for Visual Recognition">
<div> <strong>Representing Sets of Instances for Visual Recognition</strong><br>
<i>Jianxin Wu, <strong>Bin-Bin Gao</strong> and Guoqing Liu </i><br>
AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), Phoenix, Arizona, USA, 2016.<br>
[ <a href="http://cs.nju.edu.cn/_upload/tpl/00/ed/237/template237/paper/AAAI2016_D3.pdf">Paper</a>  ] 
[ <a href="javascript:toggleabs('AAAI2016')">Abstrcat</a> ]
<preabs style="display: none;">
In computer vision, a complex entity such as an image or
video is often represented as a set of instance vectors, which
are extracted from different parts of that entity. Thus, it is
essential to design a representation to encode information
in a set of instances robustly. Existing methods such as FV
and VLAD are designed based on a generative perspective,
and their performances fluctuate when difference types of instance
vectors are used (i.e., they are not robust). The proposed
D3 method effectively compares two sets as two distributions,
and proposes a directional total variation distance
(DTVD) to measure their dissimilarity. Furthermore, a robust
classifier-based method is proposed to estimate DTVD robustly,
and to efficiently represent these sets. D3 is evaluated
in action and image recognition tasks. It achieves excellent
robustness, accuracy and speed.
</preabs>
[ <a href="javascript:togglebib('AAAI2016')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{wu2016representing,
  title={Representing Sets of Instances for Visual Recognition.},
  author={Wu, Jianxin and Gao, Bin-Bin and Liu, Guoqing},
  booktitle={Proceedings of the 30th AAAI Conference on Artificial Intelligence},
  pages={2237--2243},
  year={2016}
}
</pre>
<br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ICCV15_AGE"><img class="paper" 
src="./Imgs_files/ICCV15_AGE.png" 
title="Deep Label Distribution Learning for Apparent Age Estimation ">
<div> <strong>Deep Label Distribution Learning for Apparent Age Estimation </strong><br>
<i>Xu Yang, <strong>Bin-Bin Gao</strong>, Chao Xing, Zeng-Wei Huo, Xiu-Shen Wei, Ying Zhou, Jianxin Wu and Xin Geng</i> <br>
ICCV’15 ChaLearn Looking at People workshop, Santiago, Chile, 2015.<br>
[ <a href="./Pub_files/iccvw15_AGE.pdf">Paper</a>  ]  
[ <a href="./Pub_files/iccvw15_AGE_slide.pdf">Slides</a> ] 
[ <a href="javascript:toggleabs('ICCV15_AGE')">Abstrcat</a> ]
<preabs style="display: none;">
In the age estimation competition organized by
ChaLearn, apparent ages of images are provided. Uncertainty
of each apparent age is induced because each image
is labeled by multiple individuals. Such uncertainty makes
this age estimation task different from common chronological
age estimation tasks. In this paper, we propose a
method using deep CNN (Convolutional Neural Network)
with distribution-based loss functions. Using distributions
as the training tasks can exploit the uncertainty induced by
manual labeling to learn a better model than using ages as
the target. To the best of our knowledge, this is one of the
first attempts to use the distribution as the target of deep
learning. In our method, two kinds of deep CNN models are
built with different architectures. After pre-training each
deep CNN model with different datasets as one corresponding
stream, the competition dataset is then used to fine-tune
both deep CNN models. Moreover, we fuse the results of
two streams as the final predicted ages. In the final testing
dataset provided by competition, the age estimation performance
of our method is 0.3057, which is significantly better
than the human-level performance (0.34) provided by the
competition organizers.
</preabs>
[ <a href="javascript:togglebib('ICCV15_AGE')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{yang2015deep,
  title={Deep label distribution learning for apparent age estimation},
  author={Yang, Xu and Gao, Bin-Bin and Xing, Chao and Huo, Zeng-Wei and Wei, Xiu-Shen and Zhou, Ying and Wu, Jianxin and Geng, Xin},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision Workshops},
  pages={102--108},
  year={2015}
}
</pre>
<br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ICCV15_CER"><img class="paper" 
src="./Imgs_files/ICCV15_CER.png" 
title="Deep Spatial Pyramid Ensemble for Cultural Event  Recognition">
<div> <strong>Deep Spatial Pyramid Ensemble for Cultural Event  Recognition</strong><br>
<i>Xiu-Shen Wei, <strong>Bin-Bin Gao</strong> and Jianxin Wu</i><br>
ICCV’15 ChaLearn Looking at People workshop, Santiago, Chile, 2015.<br>
[ <a href="./Pub_files/iccvw15_CER.pdf">Paper</a>  ]  
[ <a href="./Pub_files/iccvw15_CER_slide.pdf">Slides</a> ] 
[ <a href="javascript:toggleabs('ICCV15_CER')">Abstrcat</a> ]
<preabs style="display: none;">
Semantic event recognition based only on image-based
cues is a challenging problem in computer vision. In order
to capture rich information and exploit important cues
like human poses, human garments and scene categories,
we propose the Deep Spatial Pyramid Ensemble framework,
which is mainly based on our previous work, i.e., Deep Spatial
Pyramid (DSP). DSP could build universal and powerful
image representations from CNN models. Specifically,
we employ five deep networks trained on different data
sources to extract five corresponding DSP representations
for event recognition images. For combining the complementary
information from different DSP representations, we
ensemble these features by both “early fusion” and “late
fusion”. Finally, based on the proposed framework, we
come up with a solution for the track of the Cultural Event
Recognition competition at the ChaLearn Looking at People
(LAP) challenge in association with ICCV 2015. Our
framework achieved one of the best cultural event recognition
performance in this challenge.
</preabs>
[ <a href="javascript:togglebib('ICCV15_CER')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{wei2015deep,
  title={Deep spatial pyramid ensemble for cultural event recognition},
  author={Wei, Xiu-Shen and Gao, Bin-Bin and Wu, Jianxin},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision Workshops},
  pages={38--44},
  year={2015}
}
</pre>
<br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ICMLA15"><img class="paper" 
src="./Imgs_files/ICMLA15.png" 
title="Coordinate Descent Fuzzy Twin Support Vector Machine for Classification">
<div> <strong>Coordinate Descent Fuzzy Twin Support Vector Machine for Classification</strong><br>
<i><strong>Bin-Bin Gao</strong>, Jian-Jun Wang, Yao Wang and Chan-Yun Yang</i><br>
IEEE 14th International Conference on Machine Learning and Applications, Miami, Florida, USA, 2015, pp. 7--12.<br>
[ <a href="./Pub_files/icmla15_FTWSVM.pdf">Paper</a>  ] 
[ <a href="https://github.com/gaobb/CDFTSVM">Code</a> ]
[ <a href="javascript:toggleabs('ICMLA15')">Abstrcat</a> ]
<preabs style="display: none;">
In this paper, we develop a novel coordinate descent
fuzzy twin SVM (CDFTSVM) for classification. The proposed
CDFTSVM not only inherits the advantages of twin SVM but
also leads to a rapid and robust classification results. Specifically,
our CDFTSVM has two distinguished advantages: (1) An effective
fuzzy membership function is produced for removing the noise
incurred by the contaminant inputs. (2) A coordinate descent
strategy with shrinking by active set is used to deal with the
computational complexity brought by the high dimensional input.
In addition, a series of simulation experiments are conducted to
verify the performance of the CDFTSVM, which further supports
our previous claims.
</preabs>
[ <a href="javascript:togglebib('ICMLA15')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{gao2015coordinate,
  title={Coordinate Descent Fuzzy Twin Support Vector Machine for Classification},
  author={Gao, Bin-Bin and Wang, Jian-Jun and Wang, Yao and Yang, Chan-Yun},
  booktitle={IEEE 14th International Conference on Machine Learning and Applications (ICMLA)},
  pages={7--12},
  year={2015},
}
</pre>
<br>
</div>
<div class="spanner"></div>
</div>
 
</div>
</div>


<div class="section">
<h2 id="confpapers">Technical Reports</h2>

<div class="paper" id="DSP">
<img class="paper" src="./Imgs_files/DSP.png" 
title="Deep Spatial Pyramid: The Devil is Once Again in the Details">
<div> <strong>Deep Spatial Pyramid: The Devil is Once Again in the Details</strong><br>
<i><strong>Bin-Bin Gao</strong>, Xiu-Shen Wei, Jianxin Wu, Weiyao Lin</i><br>
arXiv:1504.05277v2, 2015. <br>
[ <a href="https://arxiv.org/pdf/1504.05277v2.pdf">Paper</a>  ] 
[ <a href="https://github.com/gaobb/DSP">Code</a> ]
[ <a href="javascript:toggleabs('DSP')">Abstrcat</a> ]
<preabs style="display: none;">
In this paper we show that by carefully making good
choices for various detailed but important factors in a visual
recognition framework using deep learning features,
one can achieve a simple, efficient, yet highly accurate image
classification system. We first list 5 important factors,
based on both existing researches and ideas proposed in this
paper. These important detailed factors include: 1) `2 matrix
normalization is more effective than unnormalized or `2
vector normalization, 2) the proposed natural deep spatial
pyramid is very effective, and 3) a very small K in Fisher
Vectors surprisingly achieves higher accuracy than normally
used large K values. Along with other choices (convolutional
activations and multiple scales), the proposed
DSP framework is not only intuitive and efficient, but also
achieves excellent classification accuracy on many benchmark
datasets. For example, DSP’s accuracy on SUN397 is
59.78%, significantly higher than previous state-of-the-art
(53.86%).
</preabs>
[ <a href="javascript:togglebib('DSP')">BibTeX</a> ]
<pre style="display: none;">@article{GaoDSP15,
  author    = {Gao, Bin-Bin and Wei, Xiu-Shen and Wu, Jianxin and Lin Weiyao},
  title     = {Deep Spatial Pyramid: The Devil is Once Again in the Details},
  journal   = {CoRR},
  volume    = {abs/1504.05277},
  year      = {2015},
  url       = {http://arxiv.org/abs/1504.05277},
}
</pre>
<br>
</div>
<div class="spanner"></div>
</div>
</div>
</div>


<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Contests</h2>
<div class="paper">
<ul>
<li><strong>ChaLearn Looking at People Challenge: <a href="http://gesture.chalearn.org/">Cultural Event Classification</a></strong> (<small>in conjunction with ICCV 2015</small>),  <strong>Rank</strong>: 2rd place</li>
<li><strong>ChaLearn Looking at People Challenge: <a href="http://gesture.chalearn.org/">Apprament Age Estimation</a></strong> (<small>in conjunction with ICCV 2015</small>),  <strong>Rank</strong>: 4rd place</li>
</ul>
<div class="spanner"></div>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Awards</h2>
<div class="paper">
<ul>
<li><strong>Nanruijibao Scholarship</a></strong> in Nanjing University, 2016.</li>
<li><strong>First runner-up</a></strong> in Cultural Event Recognition at ICCV 2015.(with Xiu-Shen Wei and Jianxin Wu)</li>
<li><strong>Second-class Academic Scholarship</a></strong> of Nanjing University, 2014-2015 & 2015-2016.</li>
<li><strong>Outstanding Thesis Award</a></strong> of Southwest University, 2013.</li>
<li><strong>Meritorious Winner</a></strong> of Certificate Authority Cup Mathematical Contest in Modeling, 2012.(with Qiu-Lin Li and Hong-Yan Yang)</li>
<li><strong>First-class Academic Scholarship</a></strong> of Southwest University, 2011-2012.</li>
<li><strong>Second Prize</a></strong> in China Graduate Mathematical Contest in Modeling (CGMCM), 2011.(with Qiu-Lin Li and Ji-Lian Guo)</li>
<li><strong>Outstanding Undergraduates Awards</a></strong>, 2010.</li>
<li><strong>Third Prize</a></strong> in China Undergraduate Mathematical Contest (Mathematics, Finals) (CMC), 2010.</li>
<li><strong>First Prize</a></strong> in China Undergraduate Mathematical Contest (Mathematics, Preliminaries) (CMC), 2009.</li>
<li><strong>National Scholarship for Encouragement</a></strong>, 2007-2008 & 2008-2009.</li>
</ul>
<div class="spanner"></div>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Teaching Assistants</h2>
<div class="paper">
<ul>
<li><strong><a href="https://cs.nju.edu.cn/wujx/teaching_PR.html">Pattern Recognition</a></strong> (<small>for undergraduate and graduated students. Spring, 2017.</small>)</li>
<ol> 
<li><P> &nbsp; <a href="http://cs.nju.edu.cn/wujx/teaching_PR.html">Course Page</a></P></li>
<li><P> &nbsp; <a href="./teaching.html">Assignments Page</a></P></li>
</ol>
<li><strong><a href="https://cs.nju.edu.cn/tb/prob.htm">Probability and Statistics</a></strong> (<small>for undergraduate students. Spring, 2017.</small>)</li>
<ol> 
<li><P> &nbsp; <a href="https://cs.nju.edu.cn/tb/prob.htm">Course Page</a></P></li>
<li><P> &nbsp; <a href="./teaching.html">Assignments Page</a></P></li>
</ul>
<div class="spanner"></div>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Professional Activities</h2>
<div class="paper">
<ul>
<p><font size="5">Reviewer of International Conference on Computer Vision (ICCV)</font></p>
<p><font size="5">Reviewer of Conference on Computer Vision and Pattern Recognition (CVPR)</font></p>
<p><font size="5">Reviewer of AAAI Conference on Artificial Intelligence (AAAI)</font></p>
<p><font size="5">Reviewer of Elsevier Journal of Neural Networks (NN)</font></p>
</ul>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Correspondence</h2>
<div class="paper">
<ul>
<p><font size="5"><strong>Email:<a href="mailto:csgaobb@gmail.com"></strong> csgaobb@gmail.com</a></font></p>
<p><font size="5">Bin-Bin Gao</font></p>
<p><font size="5">National Key Laboratory for Novel Software Technology</font></p>
<p><font size="5">Nanjing University</font></p>
<p><font size="5">Nanjing 210023, China</font></p>
<p><font size="5">913, Laboratory: Computer Science Building, Xianlin Campus of Nanjing University</font></p>
<p><font size="5"><a href="http://lamda.nju.edu.cn/gaobb/"></strong>Lamda homepage</a></font></p>
<p><font size="5"><a href="http://csgaobb.github.io"></strong>Github homepage</a></font></p>

</ul>
</div>
</div>
</div>

<div style="clear: both;">
<div class="smallsection">
     <!-- lamda.nju.edu.cn/gaobb-->
     <!--<script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=SPmghZAygzRWO5bn8cZHpTvhY_dkyj3w_HGM3_T4EfQ"></script>-->
     <!-- csgaobb.github.io-->
     <script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=lFQx-PXyHBymj3FqoTTJWIaTiAap63Cpqt0uNyBYmBU"></script>
</div>
</div>

<div style="clear:both;">
<p align="right"><font size="5"><a href="https://pages.github.com/">Updated on Jun. 6, 2017. Published with GitHub Pages</a></font></p>
</div>

<hr>
<div id="google_translate_element"></div>
<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({pageLanguage: 'en', layout: google.translate.TranslateElement.InlineLayout.SIMPLE}, 'google_translate_element');
}
</script><script type="text/javascript" src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script>
</body>
</html>
