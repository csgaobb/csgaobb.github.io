<!-- saved from url=(0031)https://csgaobb.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
<title>Bin-Bin Gao's homepage</title>
<link rel="shortcut icon" href="./Imgs_files/binbin_favicon.ico">
<meta content="Bin-Bin Gao, È´òÊñåÊñåÔºåcsgaobb.github.io" name="keywords">
<style media="screen" type="text/css"></style>

<link href="./main_files/homepage.css" rel="stylesheet" type="text/css">
<!--<link rel="stylesheet" href="./main_files/owl.carousel.css">
<link rel="stylesheet" href="./main_files/owl.theme.css">-->

<!-- Google Analytics -->
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-69879514-1']);
    _gaq.push(['_trackPageview']);

    (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>

</script>
<script src='./main_files/hidebib.js' type="text/javascript"></script>
</head>
<!-- Google custom search -->
  <script type="text/javascript">
    (function() {
    var cx = '015564147751910566765:wsger_p3dsg';
    var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
    '//www.google.com/cse/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
    })();
  </script>


<body>
<div class="searchbox">
     <gcse:searchbox-only resultsUrl="./search.html"
           autoCompleteMaxCompletions="5"
           autoCompleteMatchType="any"/>
     </gcse:searchbox-only>
    <a href="https://bestimage.qq.com/" target="_blank">
    <img title="YouTuLab"  class="youtu" src="./Imgs_files/youtu.png">
    </a>
    <!--
    <a href="http://www.nju.edu.cn" target="_blank">
    <img title="Nanjing University"  class="nju" src="./Imgs_files/nju.jpg">
    </a>
    <a href="http://www.lamda.nju.edu.cn/" target="_blank">
    <img title="LAMDA"  class="lamda" src="./Imgs_files/lamda.png">
    </a>
    -->
</div>
<div style="margin-bottom: 0em; border: 0px solid #ddd; background-color: #fff; padding: 1em; height: 235px;">
<div style="margin: 0px auto; width: 100%;">
<a href="https://csgaobb.github.io/" target="_blank">
<!--<img title="Bin-Bin Gao" class="myphoto1" src="./Imgs_files/IMG_5241.JPG">-->
<img title="Bin-Bin Gao" class="myphoto1"  src="./Imgs_files/gaobb.JPG">
</a>
<div style="padding-left: 15em; padding-right: 2em;vertical-align: top; height: 180px;">
<span style="line-height: 100%; font-size: 20pt;padding-top: 5em">Bin-Bin GaoÔºàÈ´òÊñåÊñåÔºâ</span><br></br>
<span>Ph.D., Senior Researcher</span> <br></br>
<span><a href="https://ai.qq.com/hr/youtu.shtml">YouTu Lab, </a></span> 
<span><a href="http://www.tencent.com/en-us/index.html">Tencent</a></span> <br>
<span><a href="http://www.lamda.nju.edu.cn/"><s>LAMDA Group, </s></a></span> 
<!--<span><a href="https://keysoftlab.nju.edu.cn/">National Key Laboratory for Novel Software Technology</a></span> <br>
<span><a href="http://cs.nju.edu.cn/">Department of Computer Science &amp; Technology</a></span> <br>
-->
<span><a href="http://nju.edu.cn/"><s>Nanjing University</s></a></span> <br></br>
<!--<span>Supervisor: <a href="http://cs.nju.edu.cn/wujx"> Prof. Jianxin Wu</a></span> <br><br>-->
<span>Shenzhen 518057, China</span><br>
<!--<span>Nanjing 210023, China</span><br>-->
<span><strong>Email: <a href="mailto:csgaobb@gmail.com"></strong>csgaobb@gmail.com</a>  or <a href="mailto:gaob@lamda.nju.edu.cn"></strong>gaobb@lamda.nju.edu.cn</a></span> <br>
<!--<a href="mailto:csgaobb@gmail.com" target="about_blank"><img src="./Imgs_files/gmail.png" style="height:20px;width:27px;margin:10px;border:0;-moz-box-shadow: 0px 0px 0px #888;
  -webkit-box-shadow: 0px 0px 0px #888;
  box-shadow: 0px 0px 0px #888;"></a><-->

<a href="https://github.com/gaobb" target="about_blank"><img src="./Imgs_files/github.svg" style="height:25px;width:25px;margin:10px;border:0;-moz-box-shadow: 0px 0px 0px #888;
  -webkit-box-shadow: 0px 0px 0px #888;
  box-shadow: 0px 0px 0px #888;"></a> 
<a href="https://weibo.com/csgaobb" target="about_blank"><img src="./Imgs_files/weibo.png" style="height:25px;width:25px;margin:10px;border:0;-moz-box-shadow: 0px 0px 0px #888;
  -webkit-box-shadow: 0px 0px 0px #888;
  box-shadow: 0px 0px 0px #888;"></a>
<a href="https://orcid.org/0000-0003-2572-8156" target="about_blank"><img src="https://orcid.org/sites/default/files/images/orcid_16x16.png" style="height:25px;width:25px;margin:10px;border:0;-moz-box-shadow: 0px 0px 0px #888;
  -webkit-box-shadow: 0px 0px 0px #888;
  box-shadow: 0px 0px 0px #888;"></a></span><br><br>
</div>
</div>
</div>
<hr>


<div style="clear: both;">
<div class="section">
<h2>Biography
<!--[<a href="./main_files/GAOBB_CV.pdf">CV</a>]--></h2>

<div style="text-align:justify;">
<div class="paper" >
I am a Senior Researcher at <a href="https://bestimage.qq.com/"> Tencent YouTu Lab</a> in Shenzhen.<!--, led by Prof. <a href="http://jiaya.me/">Jiaya Jia</a> (IEEE Fellow)--> 
Previously, I received my Ph.D. in <a href="http://www.nju.edu.cn/">Nanjing University</a> at <a href="http://www.lamda.nju.edu.cn/">LAMDA Group</a>, 
led by Prof. <a href="https://cs.nju.edu.cn/zhouzh/" target="_blank"> Zhi-Hua Zhou</a> (foreign member of the Academy of Europe, ACM/AAAI/AAAS/IEEE/IAPR Fellow), 
advised by Prof. <a href="http://cs.nju.edu.cn/wujx">Jianxin Wu</a> (member of the Thousand Talents Plan). 
Before that, I got my B.S. degree in applied mathematics in 2010, and received my M.S. degree in 2013 from <a href="http://swu.edu.cn/">Southwest University</a>, 
Chongqing China. My research interests include computer vision and machine learning, especially visual recognition and deep learning. 
I have severed as a reviewer (or PC member) for CVPR, ICCV, NeurIPS, AAAI, ECCV, ACCV, WACV, TIP, TKDE, TMI, TII, TNNLS, NN, Neurocomputing etc.
<br>
<br>
[Dec. 6, 2023] üî•üî•üî•<font color="red"> I am recruiting self-motivated interns in computer vision. If you are interested in, please directly send your CV to <a href="mailto:danylgao@tencent.com">my email: danylgao at tencent dot com</a>,
<a href="https://csgaobb.github.io/internjd.html"> [ÊãõËÅòJD]</a>.
</font>
</div>
</div>
</div>
</div>


<div style="clear: both;">
<div class="section">
  <h2>News</h2>
  <div class="paper">
    <ul>
    <li> Dec. 09, 2023: üéâüéâüéâ Three papers on anomaly detection, continual detection and matched detection are accepted by <a href="https://aaai.org/Conferences/AAAI-24/">AAAI 2024</a>.</li>  
    <li> Nov. 23, 2023: üéâüéâüéâ A paper on cross-modal continual learning is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046"> IEEE Trans. Multimedia</a>.</li>
    <li> Mar. 28, 2023: A paper on label distribution learning is accepted by <a href="https://www.editorialmanager.com/ncaa"> Neural Computing and Applications</a>.</li>
    <li> Feb. 24, 2023: A paper on few-shot image classification is accepted by <a href="https://ijcai-23.org/">IJCAI-2023</a>.</li>
    <li> Jan. 01, 2023: A paper on change detection is accepted by <a href=https://www.sciencedirect.com/journal/pattern-recognition"> Pattern Recognition</a>.</li>
    <li> Nov. 18, 2022: A paper on few-shot image classification is accepted by <a href="https://aaai.org/Conferences/AAAI-23/">AAAI 2023</a>.</li>
    <li> Sep. 15, 2022: A paper on few-shot object detection and instance segmentation is accepted by <a href=https://nips.cc/Conferences/2022/"> NeurIPS 2022</a>.</li> 
    <li> Jul. 3, 2022: A paper on few-shot image classification is accepted by <a href=https://eccv2022.ecva.net/"> ECCV 2022</a>.</li>
    <li> Jun. 30, 2022: Three papers on continual abnormal detection, few-shot and multi-label image classification are accepted by <a href="https://2022.acmmm.org/"> ACM MM 2022</a>.</li>
    <li> May. 06, 2022: A journal paper on few-shot semantic segmentation is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046"> IEEE Trans. Multimedia</a>.</li>
    <li> Jun. 05, 2021: A journal paper on multi-label image recognition is accepted by <a href="http://signalprocessingsociety.org/publications-resources/ieee-transactions-image-processing"> IEEE Trans. Image Processing</a>.</li>
    <li> Jul. 26, 2018, I joined Tencent and became a researcher of YouTu X-Lab.
    <li> Jun. 21, 2018, I got my Ph.D. in <a href="http://www.nju.edu.cn/">Nanjing University</a> at <a href="https://csgaobb.github.io/">LAMDA GROUP</a>, advised by Prof. <a href="http://cs.nju.edu.cn/wujx">Jianxin Wu</a>. 
    <li> May 21, 2018, I defended my Ph.D. thesis.
    <li> Apr. 17, 2018: A paper is accepted by <a href="https://www.ijcai-18.org/">IJCAI 2018</a> (Acceptance Rates: 710/3470=20.46%).
    <li> Mar. 28, 2018: A paper (in Chinese) is accepted by <a href="http://engine.scichina.com/publisher/scp/journal/Sci%20Sin%20Info%20F?slug=Overview">SCIENTIA SINICA Informatics</a>. 
    <li> Jul. 31, 2017: A paper accepted to <a href="http://iccv2017.thecvf.com/">ICCV 2017</a>.
    <li> Mar. 26, 2017: A journal paper on deep label distribution learning is accepted by <a href="http://signalprocessingsociety.org/publications-resources/ieee-transactions-image-processing"> IEEE Trans. Image Processing</a>.</li>
    <li> Dec. 11-17, 2015: Attended <a href="http://pamitc.org/iccv15/"> ICCV 2015</a>.</li>
    <li> Nov. 13, 2015: A paper accepted for <a href="http://aaai.org/"> AAAI 2016</a>. </li>
    <li> Oct. 10, 2015: Two papers accepted for ICCV 2015 Workshop. </li>
    <li> Sep. 20, 2015: First runner-up in <a href="http://gesture.chalearn.org/"> Cultural Event Recognition </a> at ICCV 2015. (with X.-S Wei and J. Wu)</li>
    <!--<li> Sep. 20, 2015: The fourth place in <a href="http://gesture.chalearn.org/"> Apprament Age Estimation </a> at ICCV 2015. </li>-->
    </ul>
  </div>
</div>
</div>

<!--
<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Arxiv papers</h2>


<div class="paper" id="DLDLv2-journal">
<img class="paper" 
src="./Imgs_files/DLDLv2-journal.png" 
title="Learning Expectation of Label Distribution for Facial Age and Attractiveness Estimation">
<div> <strong>Learning Expectation of Label Distribution for Facial Age and Attractiveness Estimation</strong><br>
<strong>Bin-Bin Gao</strong>, Xin-Xin Liu,Hong-Yu Zhou,Jianxin Wu,Xin Geng<br>
<i>arXiv:2007.01771, 2020. </i><br>
[ <a href="https://arxiv.org/pdf/2007.01771.pdf">Paper</a>  ]
[ <a href="javascript:toggleabs('DLDLv2-journal')">Abstrcat</a> ]
<preabs style="display: none;">
Facial attributes (e.g., age and attractiveness) estimation performance has been greatly improved by using convolutional neural networks. However, existing methods have an inconsistency between the training objectives and the evaluation metric, so they may be suboptimal. In addition, these methods always adopt image classification or face recognition models with a large amount of parameters, which carry expensive computation cost and storage overhead. In this paper, we firstly analyze the essential relationship between two state-of-the-art methods (Ranking-CNN and DLDL) and show that the Ranking method is in fact learning label distribution implicitly. This result thus firstly unifies two existing popular state-of-the-art methods into the DLDL framework. Second, in order to alleviate the inconsistency and reduce resource consumption, we design a lightweight network architecture and propose a unified framework which can jointly learn facial attribute distribution and regress attribute value. The effectiveness of our approach has been demonstrated on both facial age and attractiveness estimation tasks. Our method achieves new state-of-the-art results using the single model with 36√ó(6√ó) fewer parameters and 2.6√ó(2.1√ó) faster inference speed on facial age (attractiveness) estimation. Moreover, our method can achieve comparable results as the state-of-the-art even though the number of parameters is further reduced to 0.9M (3.8MB disk storage).
</preabs>
[ <a href="javascript:togglebib('DLDLv2-journal')">BibTeX</a> ]
<pre style="display: none;">@article{}
</pre>
<br>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="DSP">
<img class="paper" 
src="./Imgs_files/DSP.png" 
title="Deep Spatial Pyramid: The Devil is Once Again in the Details">
<div> <strong>Deep Spatial Pyramid: The Devil is Once Again in the Details</strong><br>
<strong>Bin-Bin Gao</strong>, Xiu-Shen Wei, Jianxin Wu, Weiyao Lin<br>
<i>arXiv:1504.05277v2, 2015. </i><br>
[ <a href="https://arxiv.org/pdf/1504.05277v2.pdf">Paper</a>  ]
[ <a href="https://github.com/gaobb/DSP">Code</a> ]
[ <a href="javascript:toggleabs('DSP')">Abstrcat</a> ]
<preabs style="display: none;">
In this paper we show that by carefully making good
choices for various detailed but important factors in a visual
recognition framework using deep learning features,
one can achieve a simple, efficient, yet highly accurate image
classification system. We first list 5 important factors,
based on both existing researches and ideas proposed in this
paper. These important detailed factors include: 1) `2 matrix
normalization is more effective than unnormalized or `2
vector normalization, 2) the proposed natural deep spatial
pyramid is very effective, and 3) a very small K in Fisher
Vectors surprisingly achieves higher accuracy than normally
used large K values. Along with other choices (convolutional
activations and multiple scales), the proposed
DSP framework is not only intuitive and efficient, but also
achieves excellent classification accuracy on many benchmark
datasets. For example, DSP‚Äôs accuracy on SUN397 is
59.78%, significantly higher than previous state-of-the-art
(53.86%).
</preabs>
[ <a href="javascript:togglebib('DSP')">BibTeX</a> ]
<pre style="display: none;">@article{GaoDSP15,
  author    = {Gao, Bin-Bin and Wei, Xiu-Shen and Wu, Jianxin and Lin Weiyao},
  title     = {Deep Spatial Pyramid: The Devil is Once Again in the Details},
  journal   = {CoRR},
  volume    = {abs/1504.05277},
  year      = {2015},
  url       = {http://arxiv.org/abs/1504.05277},
}
</pre>
<br>
</div>
<div class="spanner"></div>
</div>
</div>
-->

<!--<div style="clear: both;">-->
<div class="section">
<h2 id="confpapers">Selected Publications 
[<a href="http://scholar.google.com/citations?user=yYviZ-oAAAAJ&hl=en">Google Scholar</a>]</h2>




<div class="paper" id="TMM2023"><img class="paper" 
  src="./Imgs_files/ALTA-PPL.png"
  title="Cross-Modal Alternating Learning with Task-Aware Representations for Continual Learning">
  <div> <strong>Cross-Modal Alternating Learning with Task-Aware Representations for Continual Learning</strong> 
  <br><strong>Wujin Li*</strong>, <strong>Bin-Bin Gao*</strong>, Bizhong Xia, Jinbao Wang, Jun Liu, Yong Liu, Chengjie Wang and Feng Zheng<br>
  <i> IEEE Transactions on Multimedia</i>(<strong>TMM 2023</strong>), 2023. (Accepted, in press)<br>
  [ <a href="">Paper</a> ] 
  [ <a href="javascript:toggleabs('TMM2023')">Abstrcat</a> ]
  <preabs style="display: none;">
    Continual learning is a research field of artificial neural networks to simulate human lifelong learning ability. 
    Although a surge of investigations has achieved considerable performance, most rely only on image modality for 
    incremental image recognition tasks. In this paper, we propose a novel yet effective framework coined cross-modal 
    Alternating Learning with Task-Aware representations (ALTA) to make good use of visual and linguistic modal 
    information and achieve more effective continual learning. To do so, ALTA presents a cross-modal joint learning 
    mechanism that leverages simultaneous learning of image and text representations to provide more effective supervision. 
    And it mitigates forgetting by endowing task-aware representations with continual learning capability. Concurrently, 
    considering the dilemma of stability and plasticity, ALTA proposes a cross-modal alternating learning strategy that 
    alternately learns the task-aware cross-modal representations to match the image-text pairs between tasks better, 
    further enhancing the ability of continual learning. We conduct extensive experiments under various popular image 
    classification benchmarks to demonstrate that our approach achieves state-of-the-art performance. At the same time, 
    systematic ablation studies and visualization analyses validate the effectiveness and rationality of our method. 
    Our code will be available upon publication.
  </preabs>      
  [ <a href="javascript:togglebib('TMM2023')">BibTeX</a> ]
  <pre style="display: none;">@inproceedings{alta,
    title={Cross-Modal Alternating Learning with Task-Aware Representations for Continual Learning},
    author={Li, Wujin Li and Gao, Bin-Bin and Xia, Bizhong and Wang, Jinbao  and Liu, Jun and Liu, Yong and Wang, Chengjie and Zheng, Feng},
    journal={IEEE Transactions on Multimedia (TMM 2023)},
    volume={},
    number={},
    pages={},
    doi={},
    year={2023}
  }  
  </pre>
  [ <a href="https://github.com/vijaylee/ALTA">Code</a> ]
  <br>
  </div>
  <div class="spanner"></div>
  </div>



<div class="paper" id="PR2023"><img class="paper" 
  src="./Imgs_files/C-3PO.png"
  title="How to Reduce Change Detection to Semantic Segmentation">
  <div> <strong>How to Reduce Change Detection to Semantic Segmentation</strong>
  <br>Guo-Hua Wang, <strong>Bin-Bin Gao</strong> and Chengjie Wang<br>
  <i> Pattern Recognition (<strong>PR 2023</strong>),2023.</i><br>
  [ <a href="https://arxiv.org/pdf/2206.07557.pdf">Paper</a> ] 
  [ <a href="javascript:toggleabs('PR2023')">Abstrcat</a> ]
  <preabs style="display: none;">
    Change detection (CD) aims to identify changes that occur in an image pair taken different times.
    Prior methods devise specific networks from scratch to predict change masks in pixel-level, and 
    struggle with general segmentation problems. In this paper, we propose a new paradigm that reduces 
    CD to semantic segmentation which means tailoring an existing and powerful semantic segmentation 
    network to solve CD. This new paradigm conveniently enjoys the mainstream semantic segmentation 
    techniques to deal with general segmentation problems in CD. Hence we can concentrate on studying 
    how to detect changes. We propose a novel and importance insight that different change types exist 
    in CD and they should be learned separately. Based on it, we devise a module named MTF to extract 
    the change information and fuse temporal features. MTF enjoys high interpretability and reveals the 
    essential characteristic of CD. And most segmentation networks can be adapted to solve the CD problems 
    with our MTF module. Finally, we propose C-3PO, a network to detect changes at pixel-level. C-3PO achieves 
    state-of-the-art performance without bells and whistles. It is simple but effective and can be considered 
    as a new baseline in this field. Our code will be available.
  </preabs>      
  [ <a href="javascript:togglebib('PR2023')">BibTeX</a> ]
  <pre style="display: none;">@inproceedings{wangc3po,
    title={How to Reduce Change Detection to Semantic Segmentation},
    author={Wang, Guo-Hua, and Gao, Bin-Bin and Wang, Chengjie},
    journal={Pattern Recognition (PR 2023)},
    volume={138},
    number={},
    pages={},
    doi={https://doi.org/10.1016/j.patcog.2023.109384}},
    year={2023}
  }  
  </pre>
  [ <a href="https://github.com/DoctorKey/C-3PO">Code</a> ]
  <br>
  </div>
  <div class="spanner"></div>
  </div>

<div class="paper" id="NeurIPS2022"><img class="paper" 
  src="./Imgs_files/DCFS.png"
  title="Decoupling Classifier for Boosting Few-shot Object Detection and Instance Segmentation">
  <div> <strong>Decoupling Classifier for Boosting Few-shot Object Detection and Instance Segmentation</strong>
  <br><strong>Bin-Bin Gao</strong>, Xiaochen Chen, Zhongyi Huang, Congchong Nie, Jun Liu, Jinxiang Lai, Guannan Jiang, Xi Wang and Chengjie Wang<br>
  <i>In: Thirty-sixth Conference on Neural Information Processing Systems (<strong>NeurIPS 2022</strong>), New Orleans, USA, from Nov 29th to Dec 1st, 2022. </i><br>
  [ <a href="https://nips.cc/virtual/2022/poster/55372">Paper</a> ] 
  [ <a href="https://csgaobb.github.io/Projects/DCFS">ProjectPage</a> ]
  [ <a href="javascript:toggleabs('NeurIPS2022')">Abstrcat</a> ]
  <preabs style="display: none;">
    This paper focus on few-shot object detection~(FSOD) and instance segmentation~(FSIS), which
    requires a model to quickly adapt to novel classes with a few labeled instances. The existing
    methods severely suffer from bias classification because of the missing label issue which naturally
    exists in a few-shot scenario and is first formally proposed by us. Our analysis suggests that
    the standard classification head of most FSOD or FSIS models needs to be decoupled to mitigate
    the bias classification. Therefore, we propose an embarrassingly simple but effective method
    that decouples the standard classifier into two heads. Then, these two individual heads are
    capable of independently addressing clear positive samples and noisy negative samples which
    are caused by the missing label. In this way, the model can effectively learn novel classes
    while mitigating the effects of noisy negative samples. Without bells and whistles, our model
    without any additional computation cost and parameters consistently outperforms its baseline
    and state-of-the-art by a large margin on PASCAL VOC and MS-COCO benchmarks for FSOD and FSIS tasks.
    The code will be available.
  </preabs>      
  [ <a href="javascript:togglebib('NeurIPS2022')">BibTeX</a> ]
  <pre style="display: none;">@inproceedings{gao2022dc,
    title={Decoupling Classifier for Boosting Few-shot Object Detection and Instance Segmentation},
    author={Gao, Bin-Bin and Chen, Xiaochen and Huang, Zhongyi and Nie, Congchong and Liu, Jun and Lai, Jinxiang and Jiang, Guannan and Wang, Xi and Wang, Chengjie},
    booktitle={Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022)},
    volume={35},
    pages={18640--18652},
    year={2022}
  }  
  </pre>
  [ <a href="https://openreview.net/attachment?id=dVXO3Orjmxk&name=supplementary_material">[Supplementary Material]</a>]
  [ <a href="https://nips.cc/media/neurips-2022/Slides/55372_kHEysnz.pdf">[Slides]</a>]
  [ <a href="https://nips.cc/media/PosterPDFs/NeurIPS%202022/5b69b9cb83065d403869739ae7f0995e.png">[Poster]</a>]
  [ <a href="https://csgaobb.github.io/Projects/mscoco-fsod.html">FSOD-Leaderboard</a> ]
  [ <a href="https://github.com/gaobb/DCFS">Code</a> ]
  <br>
  </div>
  <div class="spanner"></div>
  </div>


<div class="paper" id="APANet">
<img class="paper" 
src="./Imgs_files/APANet.png" 
title="APANet: Adaptive Prototypes Alignment Network for Few-Shot Semantic Segmentation">
<div> <strong>APANet: Adaptive Prototypes Alignment Network for Few-Shot Semantic Segmentation</strong><br>
<strong>Jiacheng Chen*, Bin-Bin Gao*</strong>, Zongqing Lu, Jing-Hao Xue, Chengjie Wang, Qingmin Liao<br>
<i>IEEE Transactions on Multimedia</i>(<strong>TMM 2022</strong>), 2022.<br>
[ <a href="https://ieeexplore.ieee.org/document/9773019">Paper</a>  ]
[ <a href="javascript:toggleabs('APANet')">Abstrcat</a> ]
<preabs style="display: none;">
Few-shot semantic segmentation aims to segment novel-class objects in a given query image with only a few labeled support images. 
Most advanced solutions exploit a metric learning framework that performs segmentation through matching each query feature to a 
learned class-specific prototype. However, this framework suffers from biased classification due to incomplete feature comparisons. 
To address this issue, we present an adaptive prototype representation by introducing class-specific and class-agnostic prototypes 
and thus construct complete sample pairs for learning semantic alignment with query features. The complementary features learning 
manner effectively enriches feature comparison and helps yield an unbiased segmentation model in the few-shot setting. It is 
implemented with a two-branch end-to-end network (\ie, a class-specific branch and a class-agnostic branch), which generates 
prototypes and then combines query features to perform comparisons. In addition, the proposed class-agnostic branch is simple 
yet effective. In practice, it can adaptively generate multiple class-agnostic prototypes for query images and learn feature 
alignment in a self-contrastive manner. Extensive experiments on PASCAL-5i and COCO-20i demonstrate the superiority of our method. 
At no expense of inference efficiency, our model achieves state-of-the-art results in both 1-shot and 5-shot settings for semantic segmentation.
</preabs>
[ <a href="javascript:togglebib('APANet')">BibTeX</a> ]
<pre style="display: none;">@ARTICLE{APANet_TMM_2022,
  author={Chen, Jiacheng and Gao, Bin-Bin and Lu, Zongqing and Xue, Jing-Hao and Wang, Chengjie and Liao, Qingmin},
  journal={IEEE Transactions on Multimedia}, 
  title={APANet: Adaptive Prototypes Alignment Network for Few-Shot Semantic Segmentation}, 
  year={2022},
  volume={25},
  pages={4361-4373},
  doi={10.1109/TMM.2022.3174405}}
</pre>
<br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="IJCNN2021"><img class="paper" 
  src="./Imgs_files/C2F-Inst.png"
  title="A Coarse-to-Fine Instance Segmentation Network with Learning Boundary Representation">
  <div> <strong>A Coarse-to-Fine Instance Segmentation Network with Learning Boundary Representation</strong>
  <br>Feng Luo, <strong>Bin-Bin Gao</strong>, Jiangpeng Yan and Xiu Li<br>
  <i>In: Proceedings of International Joint Conference on Neural Networks (<strong>IJCNN 2021</strong>), Shenzhen, China, July 18-22, 2021, pp.1-8. </i><br>
  [ <a href="https://ieeexplore.ieee.org/abstract/document/9533399">Paper</a>  ] 
  [ <a href="javascript:toggleabs('IJCNN2021')">Abstrcat</a> ]
  <preabs style="display: none;">
    Boundary-based instance segmentation has drawn
    much attention since of its attractive efficiency. However, existing
    methods suffer from the difficulty in long-distance regression.
    In this paper, we propose a coarse-to-fine module to address
    the problem. Approximate boundary points are generated at the
    coarse stage and then features of these points are sampled and
    fed to a refined regressor for fine prediction. It is end-to-end
    trainable since differential sampling operation is well supported
    in the module. Furthermore, we design a holistic boundary-aware
    branch and introduce instance-agnostic supervision to assist
    regression. Equipped with ResNet-101, our approach achieves
    31.7% mask AP on COCO dataset with single-scale training and
    testing, outperforming the baseline 1.3% mask AP with less than
    1% additional parameters and GFLOPs. Experiments also show
    that our proposed method achieves competitive performance
    compared to existing boundary-based methods with a lightweight design and a simple pipeline.
  </preabs>      
  [ <a href="javascript:togglebib('IJCNN2021')">BibTeX</a> ]
  <pre style="display: none;">@inproceedings{luo2021coarse,
    title={A Coarse-to-Fine Instance Segmentation Network with Learning Boundary Representation},
    author={Luo, Feng and Gao, Bin-Bin and Yan, Jiangpeng and Li, Xiu},
    booktitle={International Joint Conference on Neural Networks (IJCNN)},
    pages={1--8},
    year={2021}
  }  
  </pre>
  <br>
  </div>
  <div class="spanner"></div>
  </div>


<div class="paper" id="MCAR">
<img class="paper" 
src="./Imgs_files/MCAR.png" 
title="Learning to Discover Multi-Class Attentional Regions for Multi-Label Image Recognition">
<div> <strong>Learning to Discover Multi-Class Attentional Regions for Multi-Label Image Recognition</strong><br>
<strong>Bin-Bin Gao</strong>,Hong-Yu Zhou<br>
<i>IEEE Transactions on Image Processing </i>(<strong>TIP 2021</strong>),20:5920-5932,2021.</i><br>
[ <a href="https://ieeexplore.ieee.org/document/9466402">Paper</a>  ]
[ <a href="javascript:toggleabs('MCAR')">Abstrcat</a> ]
<preabs style="display: none;">
Multi-label image recognition is a practical and challenging task compared to single-label image classification.
However, previous works may be suboptimal because of a great number of object proposals or complex attentional 
region generation modules. In this paper, we propose a simple but efficient two-stream framework to recognize
multi-category objects from global image to local regions, similar to how human beings perceive objects. 
To bridge the gap between global and local streams, we propose a multi-class attentional region module which 
aims to make the number of attentional regions as small as possible and keep the diversity of these regions as 
high as possible. Our method can efficiently and effectively recognize multi-class objects with an affordable 
computation cost and a parameter-free region localization module. Over three benchmarks on multi-label image 
classification, we create new state-of-the-art results with a single model only using image semantics without 
label dependency. In addition, the effectiveness of the proposed method is extensively demonstrated under different 
factors such as global pooling strategy, input size and network architecture.
Code has been made available at~\url{https://github.com/gaobb/MCAR}. 
</preabs>
[ <a href="javascript:togglebib('MCAR')">BibTeX</a> ]
<pre style="display: none;">@ARTICLE{MCAR_TIP_2021,
         author = {Bin-Bin Gao and Hong-Yu Zhou},
         title = {{Learning to Discover Multi-Class Attentional Regions for Multi-Label Image Recognition}},
         booktitle = {IEEE Transactions on Image Processing (TIP)},
         year={2021},
         volume={30},
         pages={5920-5932},
}
</pre>
[<a href="http://www.ccf.org.cn/xspj/rgzn/">CCF-A</a>]
[ <a href="https://github.com/gaobb/MCAR">Code</a> ]
<br>
</div>
<div class="spanner"></div>
</div>



<div class="paper" id="IJCAI2018"><img class="paper" 
src="./Imgs_files/DLDL-v2-fw.png"
title="Age Estimation Using Expectation of Label Distribution Learning">
<div> <strong>Age Estimation Using Expectation of Label Distribution Learning</strong><br>
<strong>Bin-Bin Gao</strong>, Hong-Yu Zhou, Jianxin Wu and Xin Geng<br>
<i>In: Proceedings of the 27th International Joint Conference on Artificial Intelligence (<strong>IJCAI 2018</strong>), Stockholm, Sweden, July 2018, pp.712-718. </i><br>
[ <a href="https://www.ijcai.org/proceedings/2018/0099.pdf">Paper</a>  ] 
[ <a href="./Projects/DLDL-v2.html">Project Page</a>  ]
[ <a href="javascript:toggleabs('IJCAI2018')">Abstrcat</a> ]
<preabs style="display: none;">
Age estimation performance has been greatly improved by using convolutional neural network. 
However, existing methods have an inconsistency between the training objectives and evaluation metric, 
so they may be suboptimal. In addition, these methods always adopt image classification or face recognition 
models with a large amount of parameters which bring expensive computation cost and storage overhead. 
To alleviate these issues, we design a light network architecture and propose a unified framework which 
can jointly learn age distribution and regress age. The effectiveness of our approach has been demonstrated 
on apparent and real age estimation tasks. Our method achieves new state-of-the-art results using the single
model with 36$\times$ fewer parameters and 2.6$\times$ reduction in inference time. Moreover, our method 
can achieve comparable results as the state-of-the-art even though model parameters are further reduced to 
0.9M~(3.8MB disk storage). We also analyze that Ranking methods are implicitly learning label distributions.
</preabs>      
[ <a href="javascript:togglebib('IJCAI2018')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{gaoDLDLv2,
           title={Age Estimation Using Expectation of Label Distribution Learning},
           author={Gao, Bin-Bin and Zhou, Hong-Yu and Wu, Jianxin and Geng, Xin},
           booktitle={Proceedings of the 27th International Joint Conference on Artificial Intelligence (<strong>IJCAI 2018</strong>)},
           pages={712--718},
           year={2018}
            }

</pre>
[<a href="http://www.ccf.org.cn/xspj/rgzn/">CCF-A</a>]
[<a href="https://github.com/gaobb/DLDL-v2">Code</a> ]
<br>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="SSI2018">
<a href="http://engine.scichina.com/publisher/scp/journal/SSI/doi/10.1360/N112017-00216?slug=abstract">
<img class="paper"
 src="./Imgs_files/SSI2018.png"
 title="Resource constrained deep learning: Challenges and Practices">
</a>
<div> <strong>Resource Constrained Deep Learning: Challenges and Practices (in Chinese)</strong><br>
Jianxin Wu, <strong>Bin-Bin Gao</strong>, Xiu-Shen Wei and Jian-Hao Luo<br>
<i>SCIENTIA SINICA Informatics, 48(5):501-510,2018. </i><br>
[ <a href="http://engine.scichina.com/publisher/scp/journal/SSI/doi/10.1360/N112017-00216?slug=abstract">Paper</a>  ] 
[ <a href="javascript:toggleabs('SSI2018')">Abstrcat</a> ]
<preabs style="display: none;">
Deep learning has made significant progresses in recent years. However, deep models require a lot of
computation-related resources, and its learning process needs huge number of data points and their labels. Hence,
one current research focus in deep learning is to reduce its resource consumptions, i.e., resource constrained deep
learning. In this paper, we first analyze deep learning‚Äôs thirsts for the various types of resources and the challenges
they lead to, then briefly introduce research progresses from three aspects: data, label and computation resources.
And we give detailed introductions of these areas using our research results as examples.
</preabs>      
[ <a href="javascript:togglebib('SSI2018')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{SSI2018,
           title={Resource constrained deep learning: Challenges and practices},
           author={Wu, Jianxin and Gao, Bin-Bin and Wei, Xiu-Shen and Luo, Jian-Hao},
           journal={SCIENTIA SINICA Informatics},
           volume={48},
           number={5},
           pages={501--510},
           year={2018}
            }
</pre>
<br>
</div>
<div class="spanner"></div>
</div>




<div class="paper" id="ICCV17">
<img class="paper" 
src="./Imgs_files/1AF_ICCV2017.png" 
title="Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors">
<div> <strong>Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors</strong><br>
Hong-Yu Zhou, <strong>Bin-Bin Gao</strong> and Jianxin Wu <br>
<i>In: Proceedings of the IEEE International Conference on Computer Vision </i>(<strong>ICCV 2017</strong>), Venice, Italy, October 2017, pp. 3505-3513.<br>
[ <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhou_Adaptive_Feeding_Achieving_ICCV_2017_paper.pdf">Paper</a>  ] 
[ <a href="https://funnyzhou.github.io/project/adaptive_feeding/">Project Page</a>  ]
[ <a href="javascript:toggleabs('ICCV17')">Abstrcat</a> ]
<preabs style="display: none;">
Object detection aims at high speed and accuracy simultaneously. However, fast models are usually less accurate, 
while accurate models cannot satisfy our need for speed. A fast model can be 10 times faster but 50% less accurate 
than an accurate model. In this paper, we propose Adaptive Feeding (AF) to combine a fast (but less accurate) detector 
and an accurate (but slow) detector, by adaptively determining whether an image is easy or hard and choosing an appropriate 
detector for it. In practice, we build a cascade of detectors, including the AF classifier which make the easy vs. 
hard decision and the two detectors. The AF classifier can be tuned to obtain different tradeoff between speed and accuracy, 
which has negligible training time and requires no additional training data. Experimental results on the PASCAL VOC, MS COCO 
and Caltech Pedestrian datasets confirm that AF has the ability to achieve comparable speed as the fast detector and comparable 
accuracy as the accurate one at the same time. As an example, by combining the fast SSD300 with the accurate SSD500 detector, 
AF leads to 50% speedup over SSD500 with the same precision on the VOC2007 test set.
</preabs>      
[ <a href="javascript:togglebib('ICCV17')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{zhou2017adaptive,
           title={Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors},
           author={Zhou, Hong-Yu and Gao, Bin-Bin and Wu, Jianxin},
           booktitle={Proceedings of the IEEE International Conference on Computer Vision (ICCV 2017)},
           pages={3505-3513},
           year={2017}
            }
</pre>
[<a href="http://www.ccf.org.cn/xspj/rgzn/">CCF-A</a>]
[<a href="https://github.com/funnyzhou/Adaptive_Feeding">Code</a> ]
<br>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="BMVC17">
<img class="paper" 
src="./Imgs_files/BMVC2017_SoS.png" 
title="Sunrise or Sunset: Selective Comparison Learning for Subtle Attribute Recognition">
<div> <strong>Sunrise or Sunset: Selective Comparison Learning for Subtle Attribute Recognition</strong><br>
Hong-Yu Zhou, <strong>Bin-Bin Gao</strong> and Jianxin Wu <br>
<i>In: Proceedings of the 28th British Machine Vision Conference </i>(<strong>BMVC 2017</strong>), London, UK, September 2017.<br>
[ <a href="https://arxiv.org/pdf/1707.06335.pdf">Paper</a>  ] 
[ <a href="https://funnyzhou.github.io/project/sos/">Project Page</a>  ]
[ <a href="javascript:toggleabs('BMVC17')">Abstrcat</a> ]
<preabs style="display: none;">
The difficulty of image recognition has gradually increased from general category recognition to fine-grained recognition 
and to the recognition of some subtle attributes such as temperature and geolocation. In this paper, we try to focus on 
the classification between sunrise and sunset and hope to give a hint about how to tell the difference in subtle attributes. 
Sunrise vs. sunset is a difficult recognition task, which is challenging even for humans. Towards understanding this new 
problem, we first collect a new dataset made up of over one hundred webcams from different places. Since existing algorithmic 
methods have poor accuracy, we propose a new pairwise learning strategy to learn features from selective pairs of images. 
Experiments show that our approach surpasses baseline methods by a large margin and achieves better results even compared with humans. 
We also apply our approach to existing subtle attribute recognition problems, such as temperature estimation, and 
achieve state-of-the-art results.
</preabs>      
[ <a href="javascript:togglebib('BMVC17')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{zhou2017sunrise,
            title={Sunrise or Sunset: Selective Comparison Learning for Subtle Attribute Recognition},
            author={Zhou, Hong-Yu and Gao, Bin-Bin and Wu, Jianxin},
            booktitle={Proceedings of the 28th British Machine Vision Conference (BMVC 2017)},
            year={2017}
            }
</pre>
[<a href="http://www.ccf.org.cn/xspj/rgzn/">CCF-C</a>]
<br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="TIP17">
<img class="paper" 
src="./Imgs_files/DLDL_LD.png" 
title="Deep Label Distribution Learning with Label Ambiguity ">
<div> <strong>Deep Label Distribution Learning with Label Ambiguity</strong><br>
<strong>Bin-Bin Gao</strong>, Chao Xing, Chen-Wei Xie, Jianxin Wu and Xin Geng <br>
<i>IEEE Transactions on Image Processing </i>(<strong>TIP 2017</strong>), 26(6):2825-2838,2017.<br>
[ <a href="./Pub_files/TIP2017_DLDL.pdf">Paper</a>  ] 
[ <a href="./Projects/DLDL.html">Project Page</a>  ]
[ <a href="javascript:toggleabs('TIP17')">Abstrcat</a> ]
<preabs style="display: none;">
Convolutional Neural Networks (ConvNets) have achieved excellent recognition performance in various visual recognition tasks. 
A large labeled training set is one of the most important factors for its success. However, it is difficult to collect sufficient 
training images with precise labels in some domains such as apparent age estimation, head pose estimation, multi-label 
classification and semantic segmentation. Fortunately, there is ambiguous information among labels, which makes these tasks 
different from traditional classification. Based on this observation, we convert the label of each image into a discrete label 
distribution, and learn the label distribution by minimizing a Kullback-Leibler divergence between the predicted and ground-truth 
label distributions using deep ConvNets. The proposed DLDL (Deep Label Distribution Learning) method effectively utilizes the 
label ambiguity in both feature learning and classifier learning, which prevents the network from over-fitting even when the 
training set is small. Experimental results show that the proposed approach produces significantly better results than state-of-the-art 
methods for age estimation and head pose estimation. At the same time, it also improves recognition performance for multi-label 
classification and semantic segmentation tasks.
</preabs>      
[ <a href="javascript:togglebib('TIP17')">BibTeX</a> ]
<pre style="display: none;">@ARTICLE{gao2016deep,
         author={Gao, Bin-Bin and Xing, Chao and Xie, Chen-Wei and Wu, Jianxin and Geng, Xin},
         title={Deep Label Distribution Learning with Label Ambiguity},
         journal={IEEE Transactions on Image Processing},
         year={2017},
         volume={26},
         number={6},
         pages={2825-2838}, 
         }
</pre>
[<a href="./Pub_files/TIPJCR.pdf">JCR-Q1</a>, <a href="http://www.ccf.org.cn/xspj/jsjtxxydmt/">CCF-A</a>]
[<a href="https://github.com/gaobb/DLDL">Code</a> ]
<br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="CVPR16">
<img class="paper" 
src="./Imgs_files/Fev_Lev.png" 
title="Exploit Bounding Box Annotations for Multi-label Object Recognition">
<div> <strong>Exploit Bounding Box Annotations for Multi-label Object Recognition</strong><br>
Hao Yang, Joey Tiany Zhou, Yu Zhang, <strong>Bin-Bin Gao</strong>, Jianxin Wu and Jianfei Cai<br>
<i>In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition </i>(<strong>CVPR 2016</strong>), Las Vegas, NV, USA, June 2016, pp.280-288.<br>
[ <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Exploit_Bounding_Box_CVPR_2016_paper.pdf">Paper</a>  ]
[ <a href="javascript:toggleabs('CVPR16')">Abstrcat</a> ]
<preabs style="display: none;">
Convolutional neural networks (CNNs) have shown
great performance as general feature representations for
object recognition applications. However, for multi-label
images that contain multiple objects from different categories,
scales and locations, global CNN features are not
optimal. In this paper, we incorporate local information
to enhance the feature discriminative power. In particular,
we first extract object proposals from each image. With
each image treated as a bag and object proposals extracted
from it treated as instances, we transform the multi-label
recognition problem into a multi-class multi-instance learning
problem. Then, in addition to extracting the typical
CNN feature representation from each proposal, we propose
to make use of ground-truth bounding box annotations
(strong labels) to add another level of local information
by using nearest-neighbor relationships of local regions to
form a multi-view pipeline. The proposed multi-view multiinstance
framework utilizes both weak and strong labels
effectively, and more importantly it has the generalization
ability to even boost the performance of unseen categories
by partial strong labels from other categories. Our framework
is extensively compared with state-of-the-art handcrafted
feature based methods and CNN based methods on
two multi-label benchmark datasets. The experimental results
validate the discriminative power and the generalization
ability of the proposed framework. With strong labels,
our framework is able to achieve state-of-the-art results in
both datasets.
</preabs>
[ <a href="javascript:togglebib('CVPR16')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{yang2016exploit,
  title={Exploit bounding box annotations for multi-label object recognition},
  author={Yang, Hao and Zhou, Joey Tianyi and Zhang, Yu and Gao, Bin-Bin and Wu, Jianxin and Cai, Jianfei},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={280--288},
  year={2016}
}
</pre>
[<a href="http://www.ccf.org.cn/xspj/rgzn/">CCF-A</a>]
<br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="AAAI2016">
<img class="paper" 
src="./Imgs_files/D3.png" 
title="Representing Sets of Instances for Visual Recognition">
<div> <strong>Representing Sets of Instances for Visual Recognition</strong><br>
Jianxin Wu, <strong>Bin-Bin Gao</strong> and Guoqing Liu <br>
<i>In: Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence </i>(<strong>AAAI 2016</strong>), Phoenix, Arizona, USA, Feb 2016, pp.2237-2243.<br>
[ <a href="http://cs.nju.edu.cn/_upload/tpl/00/ed/237/template237/paper/AAAI2016_D3.pdf">Paper</a>  ] 
[ <a href="javascript:toggleabs('AAAI2016')">Abstrcat</a> ]
<preabs style="display: none;">
In computer vision, a complex entity such as an image or
video is often represented as a set of instance vectors, which
are extracted from different parts of that entity. Thus, it is
essential to design a representation to encode information
in a set of instances robustly. Existing methods such as FV
and VLAD are designed based on a generative perspective,
and their performances fluctuate when difference types of instance
vectors are used (i.e., they are not robust). The proposed
D3 method effectively compares two sets as two distributions,
and proposes a directional total variation distance
(DTVD) to measure their dissimilarity. Furthermore, a robust
classifier-based method is proposed to estimate DTVD robustly,
and to efficiently represent these sets. D3 is evaluated
in action and image recognition tasks. It achieves excellent
robustness, accuracy and speed.
</preabs>
[ <a href="javascript:togglebib('AAAI2016')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{wu2016representing,
  title={Representing Sets of Instances for Visual Recognition.},
  author={Wu, Jianxin and Gao, Bin-Bin and Liu, Guoqing},
  booktitle={Proceedings of the theritieth AAAI Conference on Artificial Intelligence},
  pages={2237--2243},
  year={2016}
}
</pre>
[<a href="http://www.ccf.org.cn/xspj/rgzn/">CCF-A</a>]
<br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ICCV15_AGE">
<img class="paper" 
src="./Imgs_files/ICCV15_AGE.png" 
title="Deep Label Distribution Learning for Apparent Age Estimation ">
<div> <strong>Deep Label Distribution Learning for Apparent Age Estimation </strong><br>
Xu Yang, <strong>Bin-Bin Gao</strong>, Chao Xing, Zeng-Wei Huo, Xiu-Shen Wei, Ying Zhou, Jianxin Wu and Xin Geng <br>
<i>In: Proceedings of the IEEE ICCV‚Äô15 ChaLearn Looking at People workshop </i>(<strong>ICCVW 2015</strong>), Santiago, Chile, Dec 2015, pp.102-108.<br>
[ <a href="./Pub_files/iccvw15_AGE.pdf">Paper</a>  ]  
[ <a href="./Pub_files/iccvw15_AGE_slide.pdf">Slides</a> ] 
[ <a href="javascript:toggleabs('ICCV15_AGE')">Abstrcat</a> ]
<preabs style="display: none;">
In the age estimation competition organized by
ChaLearn, apparent ages of images are provided. Uncertainty
of each apparent age is induced because each image
is labeled by multiple individuals. Such uncertainty makes
this age estimation task different from common chronological
age estimation tasks. In this paper, we propose a
method using deep CNN (Convolutional Neural Network)
with distribution-based loss functions. Using distributions
as the training tasks can exploit the uncertainty induced by
manual labeling to learn a better model than using ages as
the target. To the best of our knowledge, this is one of the
first attempts to use the distribution as the target of deep
learning. In our method, two kinds of deep CNN models are
built with different architectures. After pre-training each
deep CNN model with different datasets as one corresponding
stream, the competition dataset is then used to fine-tune
both deep CNN models. Moreover, we fuse the results of
two streams as the final predicted ages. In the final testing
dataset provided by competition, the age estimation performance
of our method is 0.3057, which is significantly better
than the human-level performance (0.34) provided by the
competition organizers.
</preabs>
[ <a href="javascript:togglebib('ICCV15_AGE')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{yang2015deep,
  title={Deep label distribution learning for apparent age estimation},
  author={Yang, Xu and Gao, Bin-Bin and Xing, Chao and Huo, Zeng-Wei and Wei, Xiu-Shen and Zhou, Ying and Wu, Jianxin and Geng, Xin},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision Workshops},
  pages={102--108},
  year={2015}
}
</pre>
<br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ICCV15_CER">
<img class="paper" 
src="./Imgs_files/ICCV15_CER.png" 
title="Deep Spatial Pyramid Ensemble for Cultural Event  Recognition">
<div> <strong>Deep Spatial Pyramid Ensemble for Cultural Event  Recognition</strong><br>
Xiu-Shen Wei, <strong>Bin-Bin Gao</strong> and Jianxin Wu<br>
<i>In: Proceedings of the IEEE ICCV‚Äô15 ChaLearn Looking at People workshop </i>(<strong>ICCVW 2015</strong>), Santiago, Chile, Dec 2015, pp.38-44.<br>
[ <a href="./Pub_files/iccvw15_CER.pdf">Paper</a>  ]  
[ <a href="./Pub_files/iccvw15_CER_slide.pdf">Slides</a> ] 
[ <a href="javascript:toggleabs('ICCV15_CER')">Abstrcat</a> ]
<preabs style="display: none;">
Semantic event recognition based only on image-based
cues is a challenging problem in computer vision. In order
to capture rich information and exploit important cues
like human poses, human garments and scene categories,
we propose the Deep Spatial Pyramid Ensemble framework,
which is mainly based on our previous work, i.e., Deep Spatial
Pyramid (DSP). DSP could build universal and powerful
image representations from CNN models. Specifically,
we employ five deep networks trained on different data
sources to extract five corresponding DSP representations
for event recognition images. For combining the complementary
information from different DSP representations, we
ensemble these features by both ‚Äúearly fusion‚Äù and ‚Äúlate
fusion‚Äù. Finally, based on the proposed framework, we
come up with a solution for the track of the Cultural Event
Recognition competition at the ChaLearn Looking at People
(LAP) challenge in association with ICCV 2015. Our
framework achieved one of the best cultural event recognition
performance in this challenge.
</preabs>
[ <a href="javascript:togglebib('ICCV15_CER')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{wei2015deep,
  title={Deep spatial pyramid ensemble for cultural event recognition},
  author={Wei, Xiu-Shen and Gao, Bin-Bin and Wu, Jianxin},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision Workshops},
  pages={38--44},
  year={2015}
}
</pre>
<br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ICMLA15">
<img class="paper" 
src="./Imgs_files/ICMLA15.png" 
title="Coordinate Descent Fuzzy Twin Support Vector Machine for Classification">
<div> <strong>Coordinate Descent Fuzzy Twin Support Vector Machine for Classification</strong><br>
<strong>Bin-Bin Gao</strong>, Jian-Jun Wang, Yao Wang and Chan-Yun Yang<br>
<i>In: Proceedings of the IEEE Conference on Machine Learning and Applications </i>(<strong>ICMLA 2015</strong>), Miami, Florida, USA, Dec 2015, pp.7-12.<br>
[ <a href="./Pub_files/icmla15_FTWSVM.pdf">Paper</a>  ] 
[ <a href="javascript:toggleabs('ICMLA15')">Abstrcat</a> ]
<preabs style="display: none;">
In this paper, we develop a novel coordinate descent
fuzzy twin SVM (CDFTSVM) for classification. The proposed
CDFTSVM not only inherits the advantages of twin SVM but
also leads to a rapid and robust classification results. Specifically,
our CDFTSVM has two distinguished advantages: (1) An effective
fuzzy membership function is produced for removing the noise
incurred by the contaminant inputs. (2) A coordinate descent
strategy with shrinking by active set is used to deal with the
computational complexity brought by the high dimensional input.
In addition, a series of simulation experiments are conducted to
verify the performance of the CDFTSVM, which further supports
our previous claims.
</preabs>
[ <a href="javascript:togglebib('ICMLA15')">BibTeX</a> ]
<pre style="display: none;">@inproceedings{gao2015coordinate,
  title={Coordinate Descent Fuzzy Twin Support Vector Machine for Classification},
  author={Gao, Bin-Bin and Wang, Jian-Jun and Wang, Yao and Yang, Chan-Yun},
  booktitle={Proceedings of the IEEE Conference on Machine Learning and Applications (ICMLA)},
  pages={7-12},
  year={2015},
}
</pre>
[ <a href="https://github.com/gaobb/CDFTSVM">Code</a> ]
<br>
</div>
<div class="spanner"></div>
</div>
<!--
<div class="paper" id="KMTSVM">
<img class="paper" 
src="./Imgs_files/KMTSVM.png" 
title="Maximum Margin Twin Support Vector Machine for Multi-Class Classification">
<div> <strong>Maximum Margin Twin Support Vector Machine for Multi-Class Classification (in Chinese)</strong><br>
<strong>Bin-Bin Gao</strong> and Jian-Jun Wang<br>
<i>Journal of Southwest China Normal University (Natural Science Edition)</i>,10(38):130-135,2013..<br>
[ <a href="./Pub_files/KMTSVM.pdf">Paper</a>  ] 
[ <a href="javascript:toggleabs('KMTSVM')">Abstrcat</a> ]
<preabs style="display: none;">
A novel maximum margin twin support vector machine for multi-class classification (K-MTSVM) is presented in this paper. 
The K-MTSVM takes structural risk minimization principle as the optimization objective to build classification model by
introducing the margin and uses a 1-versus-1-versus-rest structure to train sub-classifiers. The experimental results on 
both artificial and UCI datasets indicate that our K-MTSVM gets better generalization performance.
</preabs>
[ <a href="javascript:togglebib('KMTSVM')">BibTeX</a> ]
<pre style="display: none;">
</pre>
<br>
</div>
<div class="spanner"></div>
</div>
-->

 
</div>
</div>










<!--<div class="paper" id="FR-TSVM">
<img class="paper" 
src="./Imgs_files/FRTSVM.png" 
title="A Fast and Robust TSVM for Pattern Classification">
<div> <strong>A Fast and Robust TSVM for Pattern Classification</strong><br>
<strong>Bin-Bin Gao</strong>, Jian-Jun Wang<br>
<i>https://arxiv.org/abs/1711.05406 </i><br>
[ <a href="./Pub_files/FR-TSVM-v2.pdf">Paper</a>  ]
[ <a href="https://github.com/gaobb/FR-TSVM">Code</a> ]
[ <a href="javascript:toggleabs('FR-TSVM')">Abstrcat</a> ]
<preabs style="display: none;">
Twin support vector machine~(TSVM) is a powerful learning algorithm by solving a pair of smaller SVM-type problems. However, there are still some specific issues such as low efficiency and weak robustness when it is faced with some real applications. In this paper, we propose a Fast and Robust TSVM~(FR-TSVM) to deal with the above issues. In order to alleviate the effects of noisy inputs, we propose an effective fuzzy membership function and reformulate the TSVMs such that different input instances can make different contributions to the learning of the separating hyperplanes. To further speed up the training procedure, we develop an efficient coordinate descent algorithm with shirking to solve the involved a pair of quadratic programming problems (QPPs). Moreover, theoretical foundations of the proposed model are analyzed in details. The experimental results on several artificial and benchmark datasets indicate that the FR-TSVM not only obtains a fast learning speed but also shows a robust classification performance. Code has been made available at: <a href="https://github.com/gaobb/FR-TSVM">this url</a>.
</preabs>
[ <a href="javascript:togglebib('FR-TSVM')">BibTeX</a> ]
<pre style="display: none;">@article{GaoFRTSVM17,
  author    = {Gao, Bin-Bin and Wang, Jian-Jun},
  title     = {A Fast and Robust TSVM for Pattern Classification},
  journal   = {CoRR},
  volume    = {abs/1711.05406},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.05406},
}
</pre>
<br>
</div>
<div class="spanner"></div>
</div>
</div>
</div>
</div>


<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Awards</h2>
<div class="paper">
<ul>
<li><strong>Nanruijibao Scholarship</a></strong> in Nanjing University, 2016.</li>
<li><strong>Second-class Academic Scholarship</a></strong> of Nanjing University, 2014-2015 & 2015-2016.</li>
<li><strong>Outstanding Thesis Award</a></strong> of Southwest University, 2013.</li>
<li><strong>First-class Academic Scholarship</a></strong> of Southwest University, 2011-2012.</li>
<li><strong>Outstanding Undergraduates Awards</a></strong>, 2010.</li>
<li><strong>National Scholarship for Encouragement</a></strong>, 2007-2008 & 2008-2009.</li>
</ul>
<div class="spanner"></div>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Competitions</h2>
<div class="paper">
<ul>
<li><strong>First runner-up</a></strong> in Cultural Event Recognition at ICCV 2015.(with Xiu-Shen Wei and Jianxin Wu)</li>
<li><strong>Fourth place</a></strong> in Apprament Age Estimation at ICCV 2015. </li> 
<li><strong>Meritorious Winner</a></strong> of Certificate Authority Cup Mathematical Contest in Modeling, 2012.(with Qiu-Lin Li and Hong-Yan Yang)</li>
<li><strong>Second Prize</a></strong> in China Graduate Mathematical Contest in Modeling (CGMCM), 2011.(with Qiu-Lin Li and Ji-Lian Guo)</li>
<li><strong>Third Prize</a></strong> in China Undergraduate Mathematical Contest (Mathematics, Finals) (CMC), 2010.</li>
<li><strong>First Prize</a></strong> in China Undergraduate Mathematical Contest (Mathematics, Preliminaries) (CMC), 2009.</li>
</ul>
<div class="spanner"></div>
</div>
</div>
</div>
-->
<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Teaching Assistants</h2>
<div class="paper">
<ul>
<li><strong><a href="https://cs.nju.edu.cn/wujx/teaching_PR.html">Pattern Recognition</a></strong> (<small>for undergraduate and graduated students. Spring, 2017.</small>)</li>
<ol> 
<li><P> &nbsp; <a href="http://cs.nju.edu.cn/wujx/teaching_PR.html">Course Page</a></P></li>
<li><P> &nbsp; <a href="./teaching.html">Assignments Page</a></P></li>
</ol>
<li><strong><a href="https://cs.nju.edu.cn/tb/prob.htm">Probability and Statistics</a></strong> (<small>for undergraduate students. Spring, 2017.</small>)</li>
<ol> 
<li><P> &nbsp; <a href="https://cs.nju.edu.cn/tb/prob.htm">Course Page</a></P></li>
<li><P> &nbsp; <a href="./teaching.html">Assignments Page</a></P></li>
</ul>
<div class="spanner"></div>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Professional Activities</h2>
<div class="paper">
<ul>
<p><font size="5">Conference Reviewer or PC Member:
<li>
<a href="https://iclr.cc/Conferences/2023">ICLR 2023</a>,
<a href="http://wacv2023.thecvf.com">WACV 2023</a>,
<a href="https://aaai.org/Conferences/AAAI-22/">AAAI 2023</a>,
<a href="http://cvpr2023.thecvf.com/">CVPR 2023</a>,
<a href="https://ijcai-23.org/">IJCAI-2023</a>,
<a href="https://icml.cc/Conferences/2023"> ICML 2023</a>,
<a href="https://nips.cc/"> NeurIPS 2023</a>
</li>

<li>
<a href="https://iclr.cc/Conferences/2022">ICLR 2022</a>,
<a href="http://wacv2022.thecvf.com">WACV 2022</a>,
<a href="https://aaai.org/Conferences/AAAI-22/">AAAI 2022</a>,
<a href="http://cvpr2022.thecvf.com/">CVPR 2022</a>,
<a href="https://ijcai-22.org/">IJCAI-2022</a>,
<a href="https://nips.cc/Conferences/2022/">NeurIPS-2022</a>

</li>
<li>
<a href="http://wacv2021.thecvf.com">WACV 2021</a>,
<a href="https://aaai.org/Conferences/AAAI-21/">AAAI 2021</a>,
<a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>,
<a href="http://iccv2021.thecvf.com/">ICCV 2021</a>
</li>
<li>
<a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>,
<a href="https://aaai.org/Conferences/AAAI-20/">AAAI 2020</a>,
<a href="http://ecai2020.eu/">ECAI 2020</a>,
<a href="https://eccv2020.eu/">ECCV 2020</a>,
<a href="http://accv2020.kyoto/">ACCV 2020</a>
</li>
<li>
<a href="http://cvpr2019.thecvf.com/">CVPR 2019</a>,
<a href="http://iccv2019.thecvf.com/">ICCV 2019</a>, 
<a href="https://aaai.org/Conferences/AAAI-19/">AAAI 2019</a>
</li> 
<li>
<a href="http://cvpr2018.thecvf.com/">CVPR 2018</a>, 
<a href="https://eccv2018.org/">ECCV 2018</a>,
<a href="http://accv2018.net">ACCV 2018</a>
</li>
<li>
<a href="http://cvpr2017.thecvf.com/">CVPR 2017</a>, 
<a href="http://iccv2017.thecvf.com/">ICCV 2017</a>
</li>
<li>
<a href="http://www.aaai.org/Press/Proceedings/aaai16.php">AAAI 2016</a>
</li>
</font></p>
<p><font size="5">Journal Reviewer: 
<li><a href="https://mc.manuscriptcentral.com/tip-ieee">IEEE Transactions on Image Processing (TIP)</a></li>
<li><a href="https://www.journals.elsevier.com/neural-networks/">Neural Networks (NN)</a></li>
<li><a href="https://mc.manuscriptcentral.com/tnnls">IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</a></li>
<li><a href="https://mc.manuscriptcentral.com/tii">IEEE Transactions on Industrial Informatics (TII)</a></li>
<li><a href="https://mc.manuscriptcentral.com/tkde-cs">IEEE Transactions on Knowledge and Data Engineering (TKDE)</a></li>
<li><a href="https://www.journals.elsevier.com/neurocomputing">Neurocomputing</a></li>
<li><a href="https://www.editorialmanager.com/RTIP">Journal of Real-Time Image Processing (JRTIP)</a></li>
<li><a href="https://mc.manuscriptcentral.com/tmi-ieee">IEEE Transactions on Medical Imaging (TMI)</a></li>
<li><a href="https://www.springer.com/journal/10115">Knowledge and Information Systems (KAIA)</a></li>
<li><a href="https://www.editorialmanager.com/ncaa">Neural Computing and Applications(NCAA)</a></li>
</font></p>
</ul>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Correspondence</h2>
<div class="paper">
<ul>
<p><font size="5"><strong>Email:<a href="mailto:csgaobb@gmail.com"></strong> csgaobb@gmail.com</a></font></p>
<p><font size="5">Bin-Bin Gao</font></p>
<p><font size="5">YouTu Lab, Tencent</font></p>
<p><font size="5">18F, Malata Building, Kejizhongyi Road</font></p>
<p><font size="5">Nanshan District, Shenzhen 518057, P.R. China</font></p>
<!--
<s><p><font size="5">National Key Laboratory for Novel Software Technology</font></p></s>
<s><p><font size="5">Nanjing University</font></p></s>
<s><p><font size="5">Nanjing 210023, China</font></p></s>
<s><p><font size="5">913, Laboratory: Computer Science Building, Xianlin Campus of Nanjing University</font></p></s>
<p><font size="5"><a href="http://lamda.nju.edu.cn/gaobb/"></strong>Lamda homepage</a></font></p>
<p><font size="5"><a href="http://csgaobb.github.io"></strong>Github homepage</a></font></p>
-->
</ul>
</div>
</div>
</div>
<hr>

<!--<div style="clear: both;">
<div class="section">-->
<!-- Êù•ÂøÖÂäõCityÁâàÂÆâË£Ö‰ª£Á†Å 
<div id="lv-container" data-id="city" data-uid="MTAyMC8zMTM0MC83OTAz">
<script type="text/javascript">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
</script>
<noscript>‰∏∫Ê≠£Â∏∏‰ΩøÁî®Êù•ÂøÖÂäõËØÑËÆ∫ÂäüËÉΩËØ∑ÊøÄÊ¥ªJavaScript</noscript>
</div>
-->
<!-- CityÁâàÂÆâË£Ö‰ª£Á†ÅÂ∑≤ÂÆåÊàê -->
<!--</div>
</div>-->



<!--
<hr>
<div id="google_translate_element"></div>
<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({pageLanguage: 'en', layout: google.translate.TranslateElement.InlineLayout.SIMPLE}, 'google_translate_element');
}
</script><script type="text/javascript" src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script>
-->

<div style="clear: both;">
<div class="clustrmapsection">
     <!-- lamda.nju.edu.cn/gaobb-->
     <!--<script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=SPmghZAygzRWO5bn8cZHpTvhY_dkyj3w_HGM3_T4EfQ"></script>-->
     <!-- csgaobb.github.io-->
     <script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=lFQx-PXyHBymj3FqoTTJWIaTiAap63Cpqt0uNyBYmBU"></script>
</div>
</div>

<div style="clear:both;">
<p align="right"><font size="2"><a href="https://csgaobb.github.io/">Updated on Dec. 10, 2023.</a></font></p>
</div>

</body>
</html>
